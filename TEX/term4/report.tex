\documentclass[specialist,
               substylefile = spbu_report.rtx,
               subf,href,colorlinks=true, 12pt]{disser}

\usepackage[a4paper,
            mag=1000, includefoot,
            left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\ifpdf\usepackage{epstopdf}\fi
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{bm}
\usepackage{float}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{caption}
\DeclareMathOperator{\SSA}{SSA}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}

% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% Использовать полужирное начертание для векторов
\let\vec=\mathbf

% Включать подсекции в оглавление
\setcounter{tocdepth}{1}

\graphicspath{{fig/}}

%----------------------------------------------------------------
\begin{document}

%
% Титульный лист на русском языке
%
% Название организации
\institution{%
    Санкт-Петербургский государственный университет\\
    Прикладная математика и информатика
}

\title{Производственная практика 2 (научно-исследовательская работа)}

% Тема
\topic{Использование метода SSA в машинном обучении для прогноза временных рядов}

% Автор
\author{Ежов Федор Валерьевич}
\group{группа 20.М03-мм}
    
% Научный руководитель
\sa       {Голяндина Нина Эдуардовна\\%
           Кафедра Статистического Моделирования}
\sastatus {к.физ.-мат.н., доцент}

% Город и год
\city{Санкт-Петербург}
\date{\number2022}


\maketitle

\tableofcontents

\chapter*{Введение}
\addcontentsline{toc}{chapter}{Введение}

В статье \cite{ar1} было проведено сравнение обычного метода ANN  и гибридного метода SSA-ANN на реальных данных. Итоги сравнения показали, что гибридные методы портят результаты прогноза из-за эффекта <<data leaking>>. В предыдущих семестрах, было продемонстрирован обратный эффект на тех же данных, а также модельных. Количество методов пополнилось рекуррентными нейронными сетями RNN \cite{rnn}, GRU \cite{gru}, LSTM \cite{lstm}, а также их гибридными аналогами SSA-RNN, SSA-GRU, SSA-LSTM. В предыдущем семестре была рассмотрена методика исследования сравнения методов, которая показалась весьма успешной, особенно на реальных данных среднемесячных осадков в Индии. Анализ реальных данных и анализ модельного ряда приводил к схожим выводам, в связи с чем возникла идея при анализе реальных данных сначала строить их модель и потом исследовать методы на модели. Модель удобна тем, что в ней можно менять параметры, можно проверять устойчивости результатов, и пр.

В этом семестре была поставлена задача: улучшить методику, применить к новым данным, добавить прогноз по SSA. Методику отработать на модельных примерах. Использовать модельные примеры для объяснения результатов сравнения.

Были рассмотрены данные EOP, данные характеристики погоды в Санкт-Петербурге, а также модельные данные. В методику добавился прогноз по методу SSA, отображение результатов прогнозов, отображение выделения сигнала с помощью метода SSA, проверка устойчивости результатов.


Структура работы следующая. В главе 1 описан алгоритм препроцессинга Singular Spectrum Analysis (SSA). В главе 2 подробно описаны все методы прогнозирования, используемые в работе. В главе 3 описана основные методы и схемы, применяемые в экспериментах в области ML, описанных в главах 4 и 5. В главе 4 описаны эксперименты на модельных данных. В главе 5 описаны эксперименты на реальных данных.


\chapter{Алгоритм Singular Spectrum Analysis}

Метод SSA используется для разложение исходного ряда в сумму рядов, которые легко интерпретировать и понять их поведение. Обычно исходный ряд раскладывается в сумму трех рядов: тренд --- медленно меняющаяся компонента, сезонность --- циклическая компонента с фиксированным периодом и шум. Информацию про базовый алгоритм SSA и связанные с методом фундаментальные понятия можно найти в книге <<Analysis of time series structure: SSA and related techniques>> \cite{SSA}.

Алгоритм SSA состоит из четырех этапов:
\begin{enumerate}
	\item Построение траекторной матрицы (Вложение).
	\item SVD.
	\item Группировка первых $r$ собственных троек.
	\item Диагональное усреднение.
\end{enumerate}

Рассмотрим каждый этап подробнее. \\
Пусть $\mathsf{X}_N = (x_1, \ldots, x_{N})$ --- временной ряд, где $N > 3$. Также будем предполагать, что найдется хоть одно $x_i \neq 0$, то есть ряд не нулевой. Обычно считается, что $x_i = f(i\Delta)$ для некоторой функции $f(t)$, где $t$ --- время, а $\Delta$ --- некоторый временной интервал. \\

\section{Этап 1. Построение траекторной матрицы (Вложение)}
Выберем целое $L$ --- длина окна, такое что $1 < L < N$. Тогда $K = N - L - 1$. 
Построим вектора 
$X_i = (x_{i} , \dots, x_{i+L-1})^\mathrm{T}$, для $1 \leqslant i \leqslant K$.
Составим из векторов $X_i$ траекторную матрицу: 

$$\mathbf{X} = [X_1 : \cdots : X_K] = 
\begin{pmatrix}
	x_1& x_2& x_3& \cdots& x_{K} \\
	x_2& x_3& x_4& \cdots& x_{K+1} \\
	x_3& x_4& x_5& \cdots& x_{K+2} \\
	\vdots& \vdots& \vdots& \ddots& \vdots \\
	x_{L}& x_{L+1}& x_{L+2}& \cdots& x_{N} 
\end{pmatrix} 
.$$

Получили матрицу $\mathbf{X}$ размерностью $L \times K$, составленную из пересекающихся частей  исходного временного ряда. Можно заметить, что на побочных диагоналях стоят одинаковые числа, такая матрица называется ганкелевой. Существует взаимно-однозначное соответствие между ганкелевыми матрицами $L \times K$ и рядами длиной $N = L + K - 1$.

Операцию получения из ряда $\mathsf{X}_N$ траекторную матрицу $\mathbf{X}$ обозначим:
$$ \mathbf{X} = \mathcal{T}_L(\mathsf{X}_N), $$
обратную операцию обозначим, как $\mathcal{T}^{-1}$ соответственно.

\section{Этап 2. Singular Value Decomposition (SVD)}
На данном этапе применяется метод SVD к траекторной матрице $\mathbf{X}$. Пусть $ \mathbf{S}  = \mathbf{X}\mathbf{X}^\mathrm{T}$ и $\lambda_1 > \dotsc > \lambda_L$ --- собственные числа матрицы $\mathbf{S}$, $U_1, \dotsc, U_L$ --- ортонормированная система базисных векторов, соответствующих собственным числам. Обозначим $V_i = \dfrac{X^\mathrm{T} U_i}{\sqrt \lambda_i}$ и $d = max\{i : \lambda_i > 0\}$. Тогда сингулярное разложение матрицы $\mathbf{X}$ запишется следующим образом:

$$\mathbf{X} = \mathbf{X}_1 + \dotsc + \mathbf{X}_d \text{, где } \mathbf{X}_i = \sqrt \lambda_i U V^\mathrm{T}, $$

Набор $(\sqrt \lambda_i, U_i, V^\mathrm{T}_i)$ будем называть i-й собственной тройкой.

\section{Этап 3. Группировка первых r собственных троек}
На этапе группировки из всех значений $\{1 \dotsc d\}$ берутся первые $r$. Пусть, $I = \{1, \dotsc , r\}$, тогда результирующая матрица соответствующая группе $I$ имеет вид:
$\mathbf{X}_{I} = \mathbf{X}_{1} + \dotsc + \mathbf{X}_{r}$.

\section{Этап 4. Диагональное усреднение}
Пусть $\mathbf{Y}$ --- матрица $L \times K$ и $ L < K$. $y_{ij}$ - элементы матрицы, где $1 \leqslant i \leqslant L, \linebreak 1 \leqslant j \leqslant K$,  $N = L + K - 1$.  Диагональное усреднение преобразует матрицу $\mathbf{Y}$ в ряд $g_0, \dotsc , g_{N-1}$ по формуле:

\begin{equation*}
	g_k = 
	\begin{cases}
		\dfrac{1}{k+1} \sum \limits_{m=1}^{k+1} y_{m, k-m+2} &  \text{, для } 0 \leqslant k < L-1,\\
		\dfrac{1}{L} \sum \limits_{m=1}^L y_{m, k-m+2} &  \text{, для } L-1 \leqslant k < K,\\
		\dfrac{1}{N-k} \sum \limits_{m=k-K+2}^{N-K+1} y_{m, k-m+2} & \text{, для } K \leqslant k < N. 
	\end{cases}
\end{equation*}

Применяя диагональное усреднение к результирующей матрице группы $I$, получаем ряд $\widehat{\mathsf{F}} = (f_1, \ldots, f_{N-1})$. Ряд $\widehat{\mathsf{F}}$ назовем оценкой сигнала, полученной с помощью SSA.
Процедуру выделения сигнала с помощью SSA обозначим как: 
$$ \widehat{\mathsf{F}} = \SSA_{L, r}(\mathsf{F}), $$ 
где $L$ --- длина окна в SSA, $r$ --- количество первых собственных троек, участвующие в построении $\widehat{\mathsf{F}}$.

\chapter{Архитектуры нейронных сетей}
\label{models}
В данной главе рассматриваются нейронные сети используемые в работе. Всего рассмотрено четыре архитектуры: одна линейная (называемая ANN), и три рекуррентных нейронных сети --- RNN, LSTM, GRU.

Каждая нейронная сеть имеет параметры и гиперпараметры. Архитектура или модель нейронной сети определяется с точностью до гиперпараметров. Термин модель означает конкретный нейросетевой метод (например ANN или LSTM), не стоит путать его с моделью временного ряда. Нейронные сети оптимизируют (подгоняют) параметры для решения задачи прогнозирования сигнала ряда во время процесса <<обучения>>. Процессом обучения называется оптимизация параметров нейронной сети с помощью градиентного метода <<Back Propagation>>. Нейронные сети как и все методы машинного обучения <<обучаются с учителем>>, то есть на парах <<признаки --- предсказываемые значения>>.


\section{Artificial Neural Network (ANN)}

Artificial Neural Network (ANN) включает в себя входной слой, ряд скрытых слоев и выходной слой, каждый слой содержит несколько узлов. Считается, что ANN с одним скрытым слоем обеспечивает достаточную сложность для моделирования нелинейных взаимосвязей данных. ANN в данной работе формализуется следующим образом: 

Входные данные, на которых модель учиться делать предсказания:
$$ X = [x_1, \ldots, x_T].$$
Предсказания модели:
$$ \hat{Y} = [\hat{y}_1, \ldots, \hat{y}_R]. $$
Формула, описывающая модель:
$$ y_k = \phi_2\bigg(\sum\limits_{j=1}^{h} w^{(2)}_{jk} \phi_1(\sum\limits_{i=1}^{T} w^{(1)}_{ij} x_{i} + \theta^{(1)}_{j})  + \theta^{(2)}_k \bigg), k = [1, \ldots, R], $$
где $T$ --- размер входного вектора, на котором выполняется прогноз. $h$ --- размер скрытого слоя. $w$ и $\theta$ --- веса модели. $\phi$ --- функции активации. $R$ --- размер выходного вектора-прогноза.

Список некоторых функций активации:

\begin{enumerate}
	\item Линейная функция активации: $\phi(x) = x$.
	\item Сигмоида: $\phi(x) = \dfrac{1}{1 + e^{-x}}$.
	\item ReLU: $\phi(x) = \begin{cases} 
		0, & x < 0, \\
		x, & x \geqslant 0.
	\end{cases}$
\end{enumerate}

\section{Recurrent neural network (RNN)}

Модель recurrent neural network (RNN), использует рекурсию в своих архитектурах для решения задач, где данные содержатся в некоторой последовательности (например, временные ряды, текстовые задачи и др.). В RNN присутствует вектор скрытого слоя, служащий для <<накопления>> информации.

$X = [x_1, \ldots, x_T]$ --- вектор входных данных. 

$ J_t = [j_1, \ldots, j_h]$ --- вектор скрытого слоя в момент $t$. 

$ \widehat{Y}_T = [\hat{y}_1, \ldots, \hat{y}_R] $ --- вектор выходных данных в момент $T$. 

Следующие формулы описывают модель:

\begin{gather*} 
	J_t = f_j(U x_{t} + \theta_1 + W J_{t-1} + \theta_2), \\
	\widehat{Y}_T = f_y(V J_T + \theta_3),
\end{gather*}

где $t = [1, \ldots, T]$,  $U, V, W, \theta_i$ -- веса модели, вычисляющиеся в процессе обучения, $f_h, f_y$ -- функции активации (в большинстве случаев сигмоиды).

\section{Long short-term memory(LSTM)}

Long short-term memory (LSTM) --- разновидность рекуррентных моделей, с добавлением второго скрытого слоя, используемого для <<долгосрочной>> памяти.

$X = [x_1, \ldots, x_T]$ --- вектор входных данных. 

$ J_t = [j_1, \ldots, j_h]$ --- вектор скрытого слоя в момент $t$. 

$ C_t = [c_1, \ldots, c_J]$ --- вектор скрытого слоя в момент $t$. 

$\widehat{Y}_T = [\hat{y}_1, \ldots, \hat{y}_R] $ --- вектор выходных данных в момент $T$. 

Следующие формулы описывают модель:

\begin{gather*} 
	f_t = \sigma(W_f \cdot [J_{t-1}, x_t] + b_f), \,
	i_t = \sigma(W_i \cdot [J_{t-1}, x_t] + b_i), \\
	\widetilde{C}_t = \tanh(W_c \cdot [J_{t-1}, x_t] + b_c), \,
	C_t = f_t * C_{t_1} + i_t * \widetilde{C}_t, \\
	o_t = \sigma(W_o \cdot [J_{t-1}, x_t] + b_o), \,
	J_t = o_t * \tanh(C_t), \\
	\widehat{Y}_T = f(V J_T + b_0),
\end{gather*}

где $t = [1, \ldots, T]$,  $W, b$ --- веса модели, $\sigma(x), \tanh(x), f(x)$ --- функции активации. Оператор $*$ --- производит поэлементное умножение.

\section{Gated Recurrent Unit (GRU)}

Gated Recurrent Unit (GRU) --- модель похожая на LSTM, но без дополнительного скрытого слоя.

$X = [x_1, \ldots, x_T]$ --- вектор входных данных. 

$ J_t = [j_1, \ldots, j_h]$ --- вектор скрытого слоя в момент $t$. 

$\widehat{Y}_T = [\hat{y}_1, \ldots, \hat{y}_R] $ --- вектор выходных данных в момент $T$. 

Следующие формулы описывают модель:

\begin{gather*} 
	z_t = \sigma(W_z \cdot [J_{t-1}, x_t] + b_z), \,
	r_t = \sigma(W_r \cdot [J_{t-1}, x_t] + b_r), \\
	\widetilde{J}_t = \tanh(W \cdot [r_t * J_{t-1}, x_t] + b), \,
	J_t = (1 - z_t) * J_{t-1} + z_t * \widetilde{J}_t, \\
	Y_T = f(V J_T + b_0),
\end{gather*}

где $t = [1, \cdots, T]$,  $W, b$ --- веса модели, $\sigma(x), \tanh(x), f(x)$ --- функции активации. Оператор $*$ --- производит поэлементное умножение.

\chapter{Использование SSA в машинном обучении}
\section{Задача}
Рассмотрим $\mathsf{Z}_N$ --- временной ряд длины $N$ и задачу: с помощью модели некоторой нейронной сети $f$ на основе $T$ последовательных точек ряда $\mathsf{Z}_N$, предсказать следующие $R$ точек ряда. 

$$ [\widehat{z}_{i+T+1}, \ldots , \widehat{z}_{i+T+R}] = f ([z_{i+1}, \ldots, z_{i+T}]). $$

Считаем, что $ \mathsf{Z}_N = \mathsf{S}_N + \xi_N, $ где $\mathsf{S}_N$ -- сигнал, $\xi_N$ -- шум, случайный процесс с нулевым мат. ожиданием. Тогда возникает идея, подавать на вход методу $f$ не сам ряд, а оценку сигнала $\widehat{\mathsf{S}}_N$, полученную с помощью метода SSA. Методы $f$, которым на вход подается $\widehat{\mathsf{S}}_N$ называем гибридными. Разница между обычными и гибридными методами заключается только в данных, поступающих на вход. Таким образом решение поставленной задачи, можно разбить на несколько частей: подготовка данных, обучение методов, прогнозирование.

\section{Подготовка данных}

$\mathsf{Z}_N$ --- изначальный временной ряд длиной $N$.
Можем представить ряд в виде траекторной матрицы для длины окна $T + R$:

\begin{equation*} \mathbf{Z} = \mathcal{T}_{T+R}(\mathsf{Z}_N) =
	\begin{pmatrix} 
		z_1 & z_2 & \cdots & z_{T} & \vrule & z_{T+1} &  \cdots & z_{T+R-1} & z_{T+R} \\
		z_2 & z_3 & \cdots & z_{T+1} & \vrule & z_{T+2} &  \cdots & z_{T+R} & z_{T+R+1} \\
		\vdots & \vdots & \ddots & \vdots & \vrule & \vdots & \ddots & \vdots & \vdots  \\
		z_{N-T-R+1} & z_{N-T-R+2} & \cdots & z_{N-R} & \vrule & z_{N-R+1} &  \cdots & z_{N-1} & z_{N} \\
	\end{pmatrix}.
\end{equation*}

Матрица $\mathbf{Z}$ имеет размерность $(N - T - R + 1) \times (T + R)$. Левую часть матрицы $\mathbf{Z}$ обозначим $\mathbf{Z}^x$, правую --- $\mathbf{Z}^y$. Разобъем матрицу по строчкам на три части: train, val, test. Пусть $\tau$, $v$ и $t$ номера последних строчек в каждой соответствующей части. 
Обозначим часть матрицы $\mathbf{Z}$ с $a$ по $b$ строчку и с $c$ по $d$ столбец как $\mathbf{Z}_{a,b}^{(c, d)}$. Тогда train, val, test записываются как: $\mathbf{Z}_{train} = \mathbf{Z}_{1,\tau}^{(1, T+R)}, \mathbf{Z}_{val} = \mathbf{Z}_{\tau+T+R,v}^{(1, T+R)}, \mathbf{Z}_{test} = \mathbf{Z}_{v+T+R,t}^{(1, T+R)}$. В этих же обозначениях $\mathbf{Z}^x = \mathbf{Z}_{1,t}^{(1, T)}, \mathbf{Z}^y = \mathbf{Z}_{1,t}^{(T+1, T+R)}$.

\subsubsection{SSA-preprocessing}
\label{SSA-preprocessing}
В этом разделе опишем алгоритм предобработки SSA для тренировочной выборки. Пусть $L, r$ --- гиперпараметры, описанные в разделе <<1.1. Алгоритм SSA>>.

\begin{enumerate}
	\item Преобразуем train часть матрицы $\mathbf{Z}$ во временной ряд $\widetilde{\mathsf{Z}} = \mathcal{T}^{-1}(\mathbf{Z}_{train})$.
	\item Получим ряд $\widehat{\mathsf{Z}} = SSA_{L, r}(\widetilde{\mathsf{Z}})$.
	\item Получаем траекторную матрицу $\mathbf{\hat{Z}} = \mathcal{T}_{T + R}(\hat{\mathsf{Z}})$. Матрица $\mathbf{\hat{Z}}$ и будет результатом работы предобработки SSA для тренировочной выборки.
\end{enumerate}

Предобработка SSA для валидационной или тестовой выборок отличается от предыдущей, ввиду разных предназначений выборок. В отличие от тренировочной выборки, о которой все известно, считается, что о валидационной и тестовой выборках нет информации. В этих случаях SSA-обработку следует применять так, чтобы предыдущие значения ряда не получали информации от будущих (<<заглядывание в будущее>>).

Пусть $\mathsf{Z}_{b, e} = [z_b, z_{b+1}, \ldots, z_e]$ подряд ряда $ \mathsf{Z} $, где $ b $ --- начальный индекс, $ e $ --- конечный индекс. Пусть $ p $ --- тоже индекс ряда, такой что $b < p < e$. Следующий алгоритм описывает процедуру получения ряда $\mathsf{Z}_{p+1, e}$, обработанного с помощью SSA без <<заглядывание в будущее>>:

\begin{enumerate}
	\itemПусть есть ряд $\mathsf{Z}_{b, e}$ и задано $p$. Тогда $Q = e - p$ --- размер ряда $ \mathsf{Z}_{p+1, e} $. Пусть $ \widehat{\mathsf{Z}}_Q = (\hat{z}_1, \ldots, \hat{z}_Q) $ --- ряд размера $Q$.
	\item Для каждого $ i = [1, \ldots, Q] $ получим $$\widehat{\mathsf{Z}}^{'}_{b+i-1, p+i} = \mathrm{SSA}_{L, r}(\mathsf{Z}_{b+i-1, p+i}). $$
	Присвоим значение последнего элемента полученного ряда $\hat{z}^{'}_{p+i}$ значению ряда $\widehat{\mathsf{Z}}_Q$ с соответствующим индексом, $\hat{z}_i = \hat{z}^{'}_{p+i}$.
	\item Получили ряд $\widehat{\mathsf{Z}}_Q$ размера $ Q $, значения которого являются значениями ряда  $\mathsf{Z}_{p+1, e}$, обработанные с помощью SSA без <<заглядывания в будущее>>.
\end{enumerate}

Обозначим процедуру получения $\widehat{\mathsf{Z}}_Q = \mathrm{SSA}^{(p)}(\mathsf{Z}_{b, e})$. Заметим, что для предобработки валидационной и тестовой выборок логично взять $Q = \tau + T$ ($\tau + T$ --- длина ряда $\mathcal{T}^{-1}(\mathsf{Z}_{train}^{x})$, который является тренировочной выборкой представленным в виде ряда). Алгоритм предобработки для валидационной выборки запишется следующим образом:

\begin{enumerate}
	\item Запишем $\mathbf{Z}_{1,v}^{(1, T+1)}$ как $\mathsf{Z}_{1, v + T + 1}$.
	\item Выберем $p = \tau + T + 1$.
	\item Получим $\widehat{\mathsf{Z}}_Q = \mathrm{SSA}^{(p)}(\mathsf{Z}_{1, v + T + 1})$.
	\item Перейдем обратно к траекторной матрице $\widehat{\mathbf{Z}}_{\mathrm{val}} = \mathcal{T}_{T+1}(\widehat{\mathsf{Z}}_Q) $, которая будет результатом предобработки SSA для валидационной выборки.
\end{enumerate}

Размерность $\widehat{\mathbf{Z}}_{\mathrm{val}}$ будет совпадать с размерностью $\mathbf{Z}_{\mathrm{val}}$. Аналогичным образом строится тестовая выборка.

\section{Обучение нейронной сети}

Оптимизация параметров нейросетевой модели проводится с помощью процедуры обратного распространения ошибки на тренировочной выборке. Модель учится по $\mathbf{Z}^x_{train}$ предсказывать $\mathbf{Z}^y_{train}$. Эпоха --- цикл прохода всех строчек из тренировочной выборки матрицы $\mathbf{Z}_{train}$ в процессе обучении. Валидационная выборка используется для оценки точности модели и оптимизации параметров. Переобучение предотвращается с помощью механизма ранней остановки. Данный алгоритм останавливает обучение, если ошибка на валидационной выборке растет на протяжение некоторого количества эпох. 

Перед началом обучения нужно выбрать гиперпараметры модели и количество эпох. Так как в обучение используется алгоритм ранней остановки, возьмем заведомо большое количество эпох.

Алгоритм обучения модели после выбора архитектуры:

\begin{enumerate}
	\item Инициализация модели со случайными весами. 
	\item На тренировочной выборке $\mathbf{Z}_{train}$ оптимизируются веса $w, \theta$ с заданным количеством эпох. Модель учится по данным строчкам $\mathbf{Z}^x_{train}$ предсказывать соответствующие строчки $\mathbf{Z}^y_{train}$. Для каждой $i$-ой эпохи считается $\varepsilon_i$ --- ошибка на валидационной выборке. Для валидационной выборки $\mathbf{Z}^x_{val}$ строится прогноз $\widehat{\mathbf{Z}}^y_{val}$. Ошибка $\varepsilon_i$ получается сравнением $\widehat{\mathbf{Z}}^y_{val}$ с $\mathbf{Z}^y_{val}$ по какой-нибудь метрике (например MSE). 
	\item Срабатывает механизм ранней остановки. Присваиваем параметрам модели те, которые были получены при минимальной ошибке $\min(\varepsilon_i)$.
\end{enumerate}

\section{Прогнозирование}
После того как модель обучена, можно перейти к прогнозированию точек ряда.

\begin{enumerate}
	\item Возьмем  $\mathbf{Z}^x_{test}$ и $\mathbf{Z}^y_{test}$.
	\item Представим $\mathbf{Z}^x_{test} = [Z_{test}^{x, 1} : \cdots : Z_{test}^{x, Q}]^T, $ где $Q$ --- количество строчек в тестовой матрицы $ \mathbf{Z}_{test} $.
	\item Для каждой строчки матрицы $\mathbf{Z}^x_{test}$ получаем прогноз с помощью обученной модели. Запишем результат прогноза как матрицу $ \mathbf{\widehat{Z}}^y = [\widehat{Z}^{y, 1} : \cdots : \widehat{Z}^{y, Q}]^\mathrm{T}$.

	\item Далее можно сравнить $\mathbf{\widehat{Z}}^y $ с $\mathbf{Z}^y_{test}$ по какой-нибудь метрике.
\end{enumerate}

\section{Метрики}
С помощью метрик MSE и RMSE можно мерить размер ошибки полученного прогноза.
$$ \MSE(\mathbf{Z}^y_{test}, \mathbf{\widehat{Z}}^y) = \frac{1}{Q} \diag\bigg((\mathbf{Z}^y_{test} - \mathbf{\widehat{Z}}^y) (\mathbf{Z}^y_{test} - \mathbf{\widehat{Z}}^y)^\mathrm{T}\bigg), $$ 
$$ \RMSE(\mathbf{Z}^y_{test}, \mathbf{\widehat{Z}}^y) = \sqrt{\MSE(\mathbf{Z}^y_{test}, \mathbf{\widehat{Z}}^y)}. $$ 

\section{Методика применения и сравнения методов}
\label{comp_tactic}
В этом разделе описана методика применения и сравнения обычных и гибридных прогнозов, а также прогнозов полученных с помощью SSA.

\paragraph{Прогноз по SSA} Подберем для метода SSA параметры $L$ и $r$, которые дают наилучшие результаты на валидационной выборке. Для параметров для $L$ и $r$ зададим сетку, по которой будем перебирать комбинации параметров, оценивая ошибку прогноза на один шаг вперед с помощью метрики RMSE на валидационной выборке. Для пяти лучших пар параметров построим прогноз на тестовой выборке и получим оценку ошибки. Каждую ошибку отобразим на графике с помощью горизонтальной прямой.

\paragraph{Выбор параметров SSA в гибридных моделях} Прежде чем сравнивать обычные и гибридные модели, нужно выбрать параметры SSA для гибридных моделей. Сделать это можно двумя способами:

\begin{enumerate}
	\item Подобрать параметры на основе SSA-анализа тренировочной части ряда или исходя из других источников информации.
	\item Перебрать параметры для метода SSA и выбрать лучшие параметры, основываясь на точности прогноза методом.
\end{enumerate}

\paragraph{Обычные и гибридные методы} Для гибридных моделей фиксируем гиперпараметры SSA $L$ и $r$, выбранные заранее. Будем сравнивать методы по сетке гиперпараметров $T$ --- размер входного вектора и $h$ --- размер скрытого слоя нейронной сети (модели нейронных сетей подробно описаны в главе \ref{models}). Каждую пару $T$ и $h$ можно представить как ячейку в таблице \ref{hyperP_grid}. Для каждой ячейки получаем ошибку по метрике RMSE на тестовой выборке. Ошибку можно усреднить по столбцам или по строкам, и построить по графику зависимости ошибки от гиперпараметров $T$ или $h$ для каждого метода соответственно. Методы можно будет сравнить на графиках.

\begin{table}[h]
\caption{Сетка гиперпараметров.}
\begin{center}
		\begin{tabular}{l|l|l|l|l|}
		\cline{2-5}
		& \multicolumn{1}{c|}{$T_1$} & \multicolumn{1}{c|}{$T_2$} & \multicolumn{1}{c|}{$\cdots$} & \multicolumn{1}{c|}{$T_n$} \\ \hline
		\multicolumn{1}{|l|}{$h_1$}    & $(T_1, h_1)$               & $(T_2, h_1)$               & $\cdots$                      & $(T_n, h_1)$               \\ \hline
		\multicolumn{1}{|l|}{$h_2$}    & $(T_1, h_2)$               & $(T_2, h_2)$               & $\cdots$                      & $(T_n, h_2)$               \\ \hline
		\multicolumn{1}{|l|}{$\vdots$} & $\vdots$                   & $\vdots$                   & $\ddots$                      & $\vdots$                   \\ \hline
		\multicolumn{1}{|l|}{$h_m$}    & $(T_1, h_m)$               & $(T_2, h_m)$               & $\cdots$                      & $(T_n, h_m)$               \\ \hline
	\end{tabular}
\end{center}
\label{hyperP_grid}
\end{table}

\paragraph{Проверка устойчивости результатов}
Для того чтобы оценить устойчивость результатов, фиксируется небольшая сетка параметров $T$ и строится таблица аналогичная \ref{hyperP_grid}. Проводится $n$ реализаций для каждой ячейки из таблицы. Так как параметры в нейронных сетях инициализируется случайно, то каждый раз получается разный результат. Для каждого $T$ полученные результаты заносятся на отдельный график зависимости ошибки от параметра $h$ в виде точек. По этим графикам можно оценить устойчивость результатов.

\paragraph{Отображение прогнозов} Чтобы объяснить успешность или неуспешность прогноза и в целом посмотреть на картину прогнозирования будем строить график прогноза на фоне правильных ответов.

\paragraph{Восстановление ряда с помощью SSA} Для оценки корректности использования предобработки SSA будем строить график восстановленного сигнала с помощью SSA на фоне временного ряда.

\chapter{Модельные данные}
\label{modts}

В данном разделе на модельном ряде проведенны эксперименты с целью, показать влияние различных особенностей временных рядов и способов выбора параметров методов на результаты точности прогнозов. В разделе рассмотрено: 

\begin{enumerate}
	\item Влияние выбора параметра $r$.
	\item Влияние наличия шума во временном ряде.
\end{enumerate}

\section{Сумма двух синусов c белым шумом}
\label{edsine}

Рассмотрим следующий ряд (рис. \ref{edsine_graph}), состоящий из элементов: $$ z_i = \big(\sin(2 \pi \dfrac{i}{6} ) + 2 \cdot \sin(2 \pi \dfrac{i}{12} ) \big) + \kappa \varepsilon_i, $$ где $\varepsilon_i \in \mathrm{N}(0, 1)$, $\kappa$ задает размер шума в ряде. 

Обозначим $\mathsf{X}_N$ ряд с $\kappa = 0.3$, а $\mathsf{Z}_N$ с $\kappa = 1.5$. Далее в этой главе рассматриваем ряды: $\mathsf{X}_{650}, \; \mathsf{Z}_{650}$.

В данном ряде легко оценить сигнал, это можно увидеть на периодограмме (рис. \ref{edsine_pgram}). Шум в ряде не очень большой, хотя видно что он присутствует. Из формулы видно, что ряд стоит из двух синусов с периодами $12$ и $6$, общий период ряда $12$. Далее в экспериментах будем перебирать параметры $T$ и $L$ по сетке с шагом кратным $12$. Будем считать аналитически верными параметрами для этого ряда $L = 175, \; r = 4$. Так как ранг ряда равен $4$, а $L = 175$ удовлетворяет асимптотической разделимости.

В экспериментах будем разбивать ряд $\mathsf{Z}_{650}$ на тренировочную, валидационную, тестовую выборки по $350$, $150$, $150$ точек соответственно. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/edsine/edsine_graph}}
	\caption{Ряд суммы синусов с белым шумом. Ряд $\mathsf{Z}_{650}$.}
	\label{edsine_graph}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/edsine/edsine_pgram}}
	\caption{Периодограмма ряда суммы синусов с белым шумом. Ряд $\mathsf{Z}_{650}$.}
	\label{edsine_pgram}
\end{figure}

\subsection{Влияние выбора параметра $r$}
\label{edsinr}
Поставим задачу сравнить обычные и гибридные методы в случае разным выбранным параметром $r$ в гибридных методах. Будем проводить сравнение на ряде $\mathsf{Z}_{650}$. Разобъем наше исследование на три части: правильно выбранное $r = 4$, недостаточно большое $r = 2$, слишком большое $r = 6$. Каждую часть будем исследовать с помощью методологии, описанной в главе \ref{comp_tactic}.

\subsubsection{Прогноз по SSA}

Сравним точность прогнозирования методом SSA при разных параметрах $r$. Зададим следующую сетку параметров $L = \{12, 24, \ldots, 168\}$, $r = \{2, 4, 6\}$. Посмотрим на результаты на рисунке \ref{edsinr_ssa_forecast}. На графике видно, что наилучшие результаты достигаются при $r = 4$, худшие результаты достигаются при $r = 6$. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/ssa_comp}}
	\caption{Сумма синусов с белым шумом. Прогнозирование с помощью метода SSA.}
	\label{edsinr_ssa_forecast}
\end{figure}

Выделим лучшее $L$ для каждого $r$. Исходя из графика для всех $r$ это будет $L = 132$. Далее будем рассматривать лучшие комбинации гиперпараметров SSA вместе с $L = 175$ и $r = \{2, 4, 6\}$.

\subsubsection{Восстановление SSA}
Посмотрим, как метод SSA восстанавливает тренировочную выборку для выбранных пар на графиках ниже. На графике \ref{edsinr_rec2} видно, что метод не восстанавливает ряд полностью. У оценки сигналы маленькая амплитуда. На графике \ref{edsinr_rec4}, где $r = 4$, метод SSA очень хорошо аппроксимирует сигнал. А для $r = 6$ на графике \ref{edsinr_rec6} видно, что в оценку сигнала просочился шум, который мешает точность решать задачу прогнозирования. На графиках видно, что разницы между восстановленными рядами нету, далее будем рассматривать только параметры $r = \{ 2, 4, 6 \}$ и $L = 175$.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinerec2}}
	\caption{Сумма синусов с белым шумом. Восстановление тренировочной выборки с помощью метода SSA. $r = 2$}
	\label{edsinr_rec2}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinerec4}}
	\caption{Сумма синусов с белым шумом. Восстановление тренировочной выборки с помощью метода SSA. $r = 4$}
	\label{edsinr_rec4}
\end{figure}


\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinerec6}}
	\caption{Сумма синусов с белым шумом. Восстановление тренировочной выборки с помощью метода SSA. $r = 6$}
	\label{edsinr_rec6}
\end{figure}


\subsubsection{Сравнение обычных и гибридных методов}
Пусть задана следующая сетка параметров: $T = \{12, 24, \ldots, 144 \}$, $h = \{10, 25, \ldots, 100 \}$. Для метода SSA в гибридных моделях возьмем пары параметров, выбранные заранее.

На графиках \ref{edsinr_r2}, \ref{edsinr_r2.h}, \ref{edsinr_r4}, \ref{edsinr_r4.h}, \ref{edsinr_r6}, \ref{edsinr_r6.h} представленны результаты сравнения по сетке параметров, заданной выше. Можно заметить, что для $r = 2$ и $r = 4$ гибридные методы явно лучше, чем обычные. Для $r = 6$ результаты начинают смешиваться. В целом для $r = 2$ и $r = 4$ ситуация выглядит похоже, какие-то гибридные методы показывают результаты лучше для $r=2$, другие для $r=4$. Также для $r=4$ кривые выглядят более горизонтально, что уменьшает зависимость ошибки от выбора гиперпараметров нейронной сети. Для точности прогнозов методом SSA все осталось также, как и в предыдущей части эксперимента.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/r2}}
	\caption{Сумма синусов с белым шумом. Ошибки прогноза в зависимости от параметра $T$. $L = 175, \; r = 2$.}
	\label{edsinr_r2}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/r2.h}}
	\caption{Сумма синусов с белым шумом. Ошибки прогноза в зависимости от параметра $h$. $L = 175, \; r = 2$.}
	\label{edsinr_r2.h}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/r4}}
	\caption{Сумма синусов с белым шумом. Ошибки прогноза в зависимости от параметра $T$. $L = 175, \; r = 4$.}
	\label{edsinr_r4}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/r4.h}}
	\caption{Сумма синусов с белым шумом. Ошибки прогноза в зависимости от параметра $h$. $L = 175, \; r = 4$.}
	\label{edsinr_r4.h}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/r6}}
	\caption{Сумма синусов с белым шумом. Ошибки прогноза в зависимости от параметра $T$. $L = 175, \; r = 6$.}
	\label{edsinr_r6}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/r6.h}}
	\caption{Сумма синусов с белым шумом. Ошибки прогноза в зависимости от параметра $h$. $L = 175, \; r = 6$.}
	\label{edsinr_r6.h}
\end{figure}

\subsubsection{Отображение прогнозов}

Ниже на графиках представленны результаты прогнозирования методами. На графиках можно подтвердить выводы полученные ранее. Так на графиках для $r = 6$ видно, что в прогноз просочился шум. Прогнозы для $r = 2$ и $r = 4$ выглядят похоже.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r2_res_ann}}
	\caption{Сумма синусов с белым шумом. Прогноз для ANN и SSA-ANN. $r = 2$}
	\label{edsinr_r2_res_ann}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r2_res_rnn}}
	\caption{Сумма синусов с белым шумом. Прогноз для RNN и SSA-RNN. $r = 2$}
	\label{edsinr_r2_res_rnn}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r2_res_gru}}
	\caption{Сумма синусов с белым шумом. Прогноз для GRU и SSA-GRU. $r = 2$}
	\label{edsinr_r2_res_gru}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r2_res_lstm}}
	\caption{Сумма синусов с белым шумом. Прогноз для LSTM и SSA-LSTM. $r = 2$}
	\label{edsinr_r2_res_lstm}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r4_res_ann}}
	\caption{Сумма синусов с белым шумом. Прогноз для ANN и SSA-ANN. $r = 4$}
	\label{edsinr_r4_res_ann}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r4_res_rnn}}
	\caption{Сумма синусов с белым шумом. Прогноз для RNN и SSA-RNN. $r = 4$}
	\label{edsinr_r4_res_rnn}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r4_res_gru}}
	\caption{Сумма синусов с белым шумом. Прогноз для GRU и SSA-GRU. $r = 4$}
	\label{edsinr_r4_res_gru}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r4_res_lstm}}
	\caption{Сумма синусов с белым шумом. Прогноз для LSTM и SSA-LSTM. $r = 4$}
	\label{edsinr_r4_res_lstm}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r6_res_ann}}
	\caption{Сумма синусов с белым шумом. Прогноз для ANN и SSA-ANN. $r = 6$}
	\label{edsinr_r6_res_ann}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r6_res_rnn}}
	\caption{Сумма синусов с белым шумом. Прогноз для RNN и SSA-RNN. $r = 6$}
	\label{edsinr_r6_res_rnn}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r6_res_gru}}
	\caption{Сумма синусов с белым шумом. Прогноз для GRU и SSA-GRU. $r = 6$}
	\label{edsinr_r6_res_gru}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/edsinr_r6_res_lstm}}
	\caption{Сумма синусов с белым шумом. Прогноз для LSTM и SSA-LSTM. $r = 6$}
	\label{edsinr_r6_res_lstm}
\end{figure}

\subsubsection{Проверка устойчивости}
Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{12, 84\}$. Сетка для параметр $h$ останется прежней. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. Полученные результаты отображены на рисунках ниже. На них подтверждается, выводы сделанные ранее. Заключаем, что полученные результаты устойчивые. 

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr2.12}}
	\caption{Сумма синусов с белым шумом. Проверка устойчивости. $r = 2, \; L = 175$. $T = 12$.}
	\label{edsiner2.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr2.84}}
	\caption{Сумма синусов с белым шумом. Проверка устойчивости. $r = 2, \; L = 175$. $T = 84$.}
	\label{edsiner2.84}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr4.12}}
	\caption{Сумма синусов с белым шумом. Проверка устойчивости. $r = 4, \; L = 175$. $T = 12$.}
	\label{edsiner4.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr4.84}}
	\caption{Сумма синусов с белым шумом. Проверка устойчивости. $r = 4, \; L = 175$. $T = 84$.}
	\label{edsiner4.84}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr6.12}}
	\caption{Сумма синусов с белым шумом. Проверка устойчивости. $r = 6, \; L = 175$. $T = 12$.}
	\label{edsiner6.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr6.84}}
	\caption{Сумма синусов с белым шумом. Проверка устойчивости. $r = 6, \; L = 175$. $T = 84$.}
	\label{edsiner6.84}
\end{figure}

\subsubsection{Выводы}
На ряде $\mathsf{Z}_{650}$ было продемонстрировано сравнение обычных и гибридных методов, и метода SSA. Из полученных результатов можем сделать выводы, что для ряда с несложно выделяемым сигналом выбор аналитически верных параметров является оптимальным.

\subsection{Влияние выбора параметра $r$ для ряда с небольшим шумом}
\label{edsinr_lownoise}
Поставим задачу сравнить обычные и гибридные методы в случае разным выбранным параметром $r$ в гибридных методах на временном ряде с небольшим шумом. Эксперимент аналогичен другому, описанному в главе \ref{edsinr}. Будем проводить сравнение на ряде $\mathsf{X}_{650}$. 

\subsubsection{Прогноз по SSA}

Сравним точность прогнозирования методом SSA при разных параметрах $r$. Зададим следующую сетку параметров $L = \{12, 24, \ldots, 168\}$, $r = \{2, 4, 6\}$. Посмотрим на результаты на рисунке \ref{edsin0.3r_ssa_forecast}. На графике видно, что наилучшие результаты достигаются при $r = 4$, худшие результаты достигаются при $r = 2$. 


\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/ssa_comp}}
	\caption{Сумма синусов с небольшим белым шумом. Прогнозирование с помощью метода SSA.}
	\label{edsin0.3r_ssa_forecast}
\end{figure}

Выделим лучшее $L$ для каждого $r$. Исходя из графика для $r = \{4, 6\}$ это будет $L = 132$, для $r = 2$ будет $L = 168$. Далее будем рассматривать лучшие комбинации гиперпараметров SSA вместе с $L = 175$ и $r = \{2, 4, 6\}$.

\subsubsection{Восстановление SSA}
Посмотрим, как метод SSA восстанавливает тренировочную выборку для выбранных пар на графиках ниже. На графике \ref{edsin0.3r_rec2} видно, что метод не восстанавливает ряд полностью. Результат очень похожи на те, что в главе \ref{edsinr}. На графиках \ref{edsin0.3r_rec4}, \ref{edsin0.3r_rec6}, видно, что результаты идентичны. В главе \ref{edsinr} для $r = 6$ в сигнал попадал шум, так как в этом примере шум небольшой, это сильно не портит оценку сигнала. На графиках видно, что разницы между восстановленными рядами нету, далее будем рассматривать только параметры $r = \{ 2, 4, 6 \}$ и $L = 175$.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsinerec2}}
	\caption{Сумма синусов с небольшим белым шумом. Восстановление тренировочной выборки с помощью метода SSA. $r = 2$}
	\label{edsin0.3r_rec2}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsinerec4}}
	\caption{Сумма синусов с небольшим белым шумом. Восстановление тренировочной выборки с помощью метода SSA. $r = 4$}
	\label{edsin0.3r_rec4}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsinerec6}}
	\caption{Сумма синусов с небольшим белым шумом. Восстановление тренировочной выборки с помощью метода SSA. $r = 6$}
	\label{edsin0.3r_rec6}
\end{figure}

\subsubsection{Сравнение обычных и гибридных методов}
Пусть задана следующая сетка параметров: $T = \{12, 24, \ldots, 144 \}$, $h = \{10, 25, \ldots, 100 \}$. Для метода SSA в гибридных моделях возьмем пары параметров, выбранные заранее.

На графиках \ref{edsin0.3r_r2}, \ref{edsin0.3r_r2.h}, \ref{edsin0.3r_r4}, \ref{edsin0.3r_r4.h}, \ref{edsin0.3r_r6}, \ref{edsin0.3r_r6.h} представленны результаты сравнения по сетке параметров, заданной выше. Можно заметить, что гибридные методы явно лучше, чем обычные. Для всех $r$ результаты очень похожие. Можно заметить, что для $r = 4$ ошибка меньше, чем для других $r$, а также разрыв в точности между обычными и гибридными методами виден больше. Разрывы на графиках небольшие, так как в ряде очень слабый шум. Для точности прогнозов методом SSA все осталось также, как и в предыдущей части эксперимента.


\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/r2}}
	\caption{Сумма синусов с небольшим белым шумом. Ошибки прогноза в зависимости от параметра $T$. $L = 175, \; r = 2$.}
	\label{edsin0.3r_r2}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/r2.h}}
	\caption{Сумма синусов с небольшим белым шумом. Ошибки прогноза в зависимости от параметра $h$. $L = 175, \; r = 2$.}
	\label{edsin0.3r_r2.h}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/r4}}
	\caption{Сумма синусов с небольшим белым шумом. Ошибки прогноза в зависимости от параметра $T$. $L = 175, \; r = 4$.}
	\label{edsin0.3r_r4}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/r4.h}}
	\caption{Сумма синусов с небольшим белым шумом. Ошибки прогноза в зависимости от параметра $h$. $L = 175, \; r = 4$.}
	\label{edsin0.3r_r4.h}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/r6}}
	\caption{Сумма синусов с небольшим белым шумом. Ошибки прогноза в зависимости от параметра $T$. $L = 175, \; r = 6$.}
	\label{edsin0.3r_r6}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/r6.h}}
	\caption{Сумма синусов с небольшим белым шумом. Ошибки прогноза в зависимости от параметра $h$. $L = 175, \; r = 6$.}
	\label{edsin0.3r_r6.h}
\end{figure}

\subsubsection{Отображение прогнозов}

На графиках ниже представленны результаты прогнозирования методами. Из-за маленького шума все прогнозы похожи друг на друга.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r2_res_ann}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для ANN и SSA-ANN. $r = 2$}
	\label{edsin0.3r_r2_res_ann}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r2_res_rnn}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для RNN и SSA-RNN. $r = 2$}
	\label{edsin0.3r_r2_res_rnn}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r2_res_gru}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для GRU и SSA-GRU. $r = 2$}
	\label{edsin0.3r_r2_res_gru}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r2_res_lstm}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для LSTM и SSA-LSTM. $r = 2$}
	\label{edsin0.3r_r2_res_lstm}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r4_res_ann}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для ANN и SSA-ANN. $r = 4$}
	\label{edsin0.3r_r4_res_ann}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r4_res_rnn}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для RNN и SSA-RNN. $r = 4$}
	\label{edsin0.3r_r4_res_rnn}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r4_res_gru}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для GRU и SSA-GRU. $r = 4$}
	\label{edsin0.3r_r4_res_gru}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r4_res_lstm}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для LSTM и SSA-LSTM. $r = 4$}
	\label{edsin0.3r_r4_res_lstm}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r6_res_ann}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для ANN и SSA-ANN. $r = 6$}
	\label{edsin0.3r_r6_res_ann}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r6_res_rnn}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для RNN и SSA-RNN. $r = 6$}
	\label{edsin0.3r_r6_res_rnn}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r6_res_gru}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для GRU и SSA-GRU. $r = 6$}
	\label{edsin0.3r_r6_res_gru}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/N0.3/edsin0.3r_r6_res_lstm}}
	\caption{Сумма синусов с небольшим белым шумом. Прогноз результатов для LSTM и SSA-LSTM. $r = 6$}
	\label{edsin0.3r_r6_res_lstm}
\end{figure}

\subsubsection{Проверка устойчивости}
Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{12, 84\}$. Сетка для параметр $h$ останется прежней. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. Полученные результаты отображены на рисунках ниже. На них подтверждается, выводы сделанные ранее. Заключаем, что полученные результаты устойчивые. 


\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr2.12}}
	\caption{Сумма синусов с небольшим белым шумом. Проверка устойчивости. $r = 2, \; L = 175$. $T = 12$.}
	\label{edsine0.3r2.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr2.84}}
	\caption{Сумма синусов с небольшим белым шумом. Проверка устойчивости. $r = 2, \; L = 175$. $T = 84$.}
	\label{edsine0.3r2.84}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr4.12}}
	\caption{Сумма синусов с небольшим белым шумом. Проверка устойчивости. $r = 4, \; L = 175$. $T = 12$.}
	\label{edsine0.3r4.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr4.84}}
	\caption{Сумма синусов с небольшим белым шумом. Проверка устойчивости. $r = 4, \; L = 175$. $T = 84$.}
	\label{edsine0.3r4.84}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr6.12}}
	\caption{Сумма синусов с небольшим белым шумом. Проверка устойчивости. $r = 6, \; L = 175$.  $T = 12$.}
	\label{edsine0.3r6.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/edsine/loopr6.84}}
	\caption{Сумма синусов с небольшим белым шумом. Проверка устойчивости. $r = 6, \; L = 175$. $T = 84$.}
	\label{edsine0.3r6.84}
\end{figure}


\subsubsection{Выводы}
На ряде $\mathsf{X}_{650}$ было продемонстрировано сравнение обычных и гибридных методов, и метода SSA. Из полученных результатов можем сделать выводы, что для ряда с несложно выделяемым сигналом выбор аналитически верных параметров является оптимальным и для маленького шума. Также в эксперименте было продемонстрирована меньшая разница в точности между обычными и гибридными методами. Было показано, что выбор параметра $r$ меньше, чем нужно, ведет к большему ухудшению точности, в отличие от выбора в большую сторону.

\begin{comment}
\section{Сумма двух синусов c модулируемой амплитудой}
\label{dsine}
Рассмотрим следующий ряд $\mathsf{Z}_{N}$ (рис. \ref{pic_ndsine} для 1500 точек), состоящий из элементов: $$ z_i = \big(\sin(2 \pi \dfrac{i}{T} ) + 1.6 \sin(2 \pi \dfrac{i}{T + 2} ) \big) \cdot \big(\cos(2 \pi \dfrac{i}{432}) + 5\big) + \kappa \varepsilon_i, $$ где $T = 12$, а $\varepsilon_i$ -- шум из стандартного нормального распределения $N(0, 1)$, $\kappa$ задает размер шума в ряде. На рис. \ref{dsine_pgram} представлена периодограмма ряда для $\kappa = 0 \text{ и } N = 1500$.
Из формулы ряда видно, что он его периодика состоит из двух синусов с периодами $12$ и $14$, общий период такого ряда будет равен $13$. Ряд имеет не самый простую структуру сигнала, это можно наблюдать на периодограмме \ref{dsine_pgram}. Пики располагаются близко друг к другу, что может привести к смешиванию периодик.

Обозначим $\mathsf{X}_N$ ряд с $\kappa = 0.3$, а $\mathsf{Z}_N$ с $\kappa = 9.52$. Далее в этой главе рассматриваем ряды: $\mathsf{X}_{620}, \; \mathsf{X}_{1500}, \; \mathsf{Z}_{620}, \; \mathsf{Z}_{1500}$. В экспериментах будем разбивать ряды длиной $620$ точек на тренировочную, валидационную, тестовую выборки по $320$, $150$, $150$ точек соответственно. Ряды длиной $1500$ точек по $750$, $500$, $250$ соответственно.

Зная изначальный вид ряда зафиксируем следующие параметры SSA $r = 12$, а $L$ равной половине длины тренировочной выборки в зависимости от ряда. Будем считать эти параметры как аналитические обоснованными, так как ранг сигнала ряда равен $12$. При $L$, равной половине длины тренировочной выборки, достигается асимптотическая разделимость. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/dsine/dsineN0.3}}
	\caption{Ряд суммы синусов.}
	\label{pic_ndsine}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/dsine/dsineN0.3_pgram}}
	\caption{Периодограмма ряда суммы синусов.}
	\label{dsine_pgram}
\end{figure}

\begin{comment}

\subsection{Влияние длины ряда на точность прогноза}
\label{leninfl}
Поставим задачу проверить влияние длины ряда на точность прогноза. Будем проводить сравнение на рядах $\mathsf{Z}_{620}$ и $\mathsf{Z}_{1500}$. 

\subsubsection{Прогноз по SSA}
Зададим следующую сетку параметров $L = \{ 13, 26, \ldots, 160 \}$, $r = \{ 8, 12, 16 \}$ для ряда $\mathsf{Z}_{620}$. Для ряда $\mathsf{Z}_{1500}$ $L = \{ 13, 26, \ldots, 375 \}$, $r = \{ 8, 12, 16 \}$.

Посмотрим на результаты представленные в таблице \ref{dsineleninfl_ssa_forecast}. Из таблицы видно, что все результаты точности для ряда $\mathsf{X}_{1500}$ ниже на $0.1$, чем для ряда $\mathsf{X}_{620}$. Из этого можно сделать вывод, что увеличение длины помогает улучшить точность прогноза методом SSA. Видно, что для ряда $\mathsf{X}_{1500}$ наилучшие результаты достигаются для $r=12$ и больших $L$, можно сказать, что увеличение ряда хорошо сказывается на выделение сигнала методом. Для ряда $\mathsf{X}_{620}$ наилучшие результаты достигаются при $r = 8$ и среднем $L$. Аналитически верная пара параметров находится на $25$ месте в ранжированном списке лучших пар.

\begin{table}[H]
\caption{Лучшие пять результатов прогнозов, сделанные методом SSA}
\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\cline{1-4} \cline{6-9}
		$\mathsf{Z}_{620}$ & RMSE  & L   & r  &  & $\mathsf{Z}_{1500}$ & RMSE  & L   & r  \\ \cline{1-4} \cline{6-9} 
		1                  & 11.591 & 130 & 8   &  & 1                   & 9.735 & 234 & 8 \\ \cline{1-4} \cline{6-9} 
		2                  & 11.605 & 117 & 8  &  & 2                   & 9.74 & 221 & 8 \\ \cline{1-4} \cline{6-9} 
		3                  & 11.682 & 91  & 8  &  & 3                   & 9.751 & 168 & 8 \\ \cline{1-4} \cline{6-9} 
		4                  & 11.688 & 143 & 8 &  & 4                   & 9.757  & 208 & 8 \\ \cline{1-4} \cline{6-9} 
		5                  & 11.736 & 78  & 8  &  & 5                   & 9.758 & 182 & 8 \\ \cline{1-4} \cline{6-9} 
		14                 & 12.446 & 160 & 12 &  & 35                   & 10.01 & 375 & 12 \\ \cline{1-4} \cline{6-9} 
	\end{tabular}
\end{center}
\label{dsineleninfl_ssa_forecast}
\end{table}

Далее для каждого ряда зафиксируем две пары параметров для которых продолжим исследование. Для ряда $\mathsf{X}_{620}$ зафиксируем аналитически верную пару $L = 160, \; r = 12$ и лучшую пару из таблицы \ref{dsineleninfl_ssa_forecast} $L = 52, \; r = 8$. Для ряда $\mathsf{X}_{1500}$ аналогично зафиксируем $L = 375, \; r = 12$ и $L = 325, \; r = 12$.


\subsubsection{Восстановление SSA}

\subsubsection{Сравнение обычных и гибридных методов}

\subsubsection{Проверка устойчивости}

\subsubsection{Отображение результатов прогнозов}

\subsubsection{Выводы}




\subsection{Влияние выбора параметра $r$ в гибридных моделях}
\label{dsinr}
Поставим задачу сравнить обычные и гибридные методы в случае разным выбранным параметром $r$ в гибридных методах. Будем проводить сравнение на ряде $\mathsf{Z}_{620}$. Разобъем наше исследование на три части: правильно выбранное $r$, недостаточно большое $r$, слишком большое $r$. Каждую часть будем исследовать с помощью методологии, описанной в главе \ref{comp_tactic}.

\subsubsection{Правильно выбранное $r$}
\label{good_dsinr}
Будем считать, что правильно выбранное $r$, то которое совпадает с рангом сигнала. Для нашего ряда $r = 12$. Зафиксируем $r=12$ и сравним обычные и гибридные методы, а также метод SSA.

\subsubsection*{Прогноз по SSA}

Зададим следующую сетку параметра $L = \{ 12, 24, \ldots, 175 \}$. Зафиксируем параметр $r = 12$.

Посмотрим на таблицу \ref{good_dsinr_ssa_forecast}, на ней представленны пять лучших пар параметров SSA по заданной сетке параметров. Заметим, что аналитически верная пара попала в пятерку лучших. Также из таблицы видно, что все параметры $L$ получились большие, что намекает на правильность выбора аналитическо верной пары.

\begin{table}[H]
	\caption{Лучшие пять результатов прогнозов, сделанные методом SSA}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\cline{1-4}
			$\mathsf{Z}_{620}$ & RMSE   & L   & r  \\ \cline{1-4}
			1                  & 12.219 & 117 & 12  \\ \cline{1-4}  
			2                  & 12.324 & 104 & 12  \\ \cline{1-4} 
			3                  & 12.444 & 143  & 12  \\ \cline{1-4}  
			4                  & 12.446 & 160 & 12  \\ \cline{1-4} 
			5                  & 12.504 & 91  & 12  \\ \cline{1-4} 
		\end{tabular}
	\end{center}
	\label{good_dsinr_ssa_forecast}
\end{table}

Зафиксируем две пары параметров для которых продолжим исследование: аналитически верную пару $L = 160, \; r = 12$ и лучшую пару из таблицы \ref{ser_ssa_forecast} $L = 117, \; r = 12$. 

\subsubsection*{Восстановление SSA}
Посмотрим, как метод SSA восстанавливает тренировочную выборку для выбранных пар на графике  \ref{good_dsinr_117LH}. Из графика видно, что метод при разных $L$ производит почти идентично восстанавливает сигнал. Видно, что сигнал восстанавливает не совсем корректно, шум смешался с периодиками. Это может случиться, так как у ряда $\mathsf{Z}_{620}$ не просто сигнал, что уже отмечалось раньше.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/117LH}}
	\caption{Восстановление тренировочной выборки с помощью метода SSA.}
	\label{good_dsinr_117LH}
\end{figure}

\subsubsection*{Сравнение обычных и гибридных методов}

Пусть задана следующая сетка параметров: $T = \{13, 52, 91, 130 \}$, $h = \{10, 25, \ldots, 100 \}$. Для метода SSA в гибридных моделях возьмем пары параметров, выбранные заранее. 

Теперь посмотрим, как справляются гибридные методы с выбранными ранее параметрами. Посмотрим на рисунки \ref{good_dsinr.T}, \ref{good_dsinr.h}, \ref{good_dsinr.117T}, \ref{good_dsinr.117h}. На них видно, что гибридные модели выигрывают для $T = 13$, дальше результаты перемешиваются. На рисунках \ref{good_dsinr.h}, \ref{good_dsinr.117h} заметно превосходство гибридных моделей над обычными. Видно, что наилучших результатов достигает модель SSA-RNN. Метод SSA показывает хорошую точность для $L = 104$, также показывает одинаковую точность для $L = 91, \; 117$ (на рисунках линии наложились друг на друга). Видно, что при параметре $L = 117$ гибридные методы показали себя немного лучше. Можно говорить о улучшении точности, при выборе лучших параметров для SSA из таблицы \ref{good_dsinr_ssa_forecast}. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/T}}
	\caption{Сравнение методов. $L = 160, \; r = 12$.}
	\label{good_dsinr.T}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/h}}
	\caption{Сравнение методов. $L = 160, \; r = 12$.}
	\label{good_dsinr.h}
\end{figure}


\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/117T}}
	\caption{Сравнение методов. $L = 117, \; r = 12$.}
	\label{good_dsinr.117T}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/117h}}
	\caption{Сравнение методов. $L = 117, \; r = 12$.}
	\label{good_dsinr.117h}
\end{figure}

\subsubsection*{Проверка устойчивости}
Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{13, 91\}$. Сетка для параметр $h$ останется прежней. Для метода SSA в гибридных моделях возьмем пары параметров, выбранные заранее. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. 

Полученные результаты отображены на рис. \ref{good_dsinr_13}, \ref{good_dsinr_91}, \ref{good_dsinr_13.im}, \ref{good_dsinr_91.imp}. На графиках видно, что результаты смешиваются, но точки для гибридных методов находятся ниже, из чего можно заключить что гибридные методы показывают себя лучше. На графиках видно, что данные не однородны. Можно заключить что результаты устойчивы.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/13LH}}
	\caption{Проверка устойчивости. Пара $L = 160, \; r = 12$. $T = 13$.}
	\label{good_dsinr_13}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/91LH}}
	\caption{Проверка устойчивости. Пара $L = 160, \; r = 12$. $T = 91$.}
	\label{good_dsinr_91}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/13L117}}
	\caption{Проверка устойчивости. Пара $L = 117, \; r = 12$. $T = 13$.}
	\label{good_dsinr_13.imp}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/dsine/rinf/good/91L117}}
	\caption{Проверка устойчивости. Пара $L = 117, \; r = 12$. $T = 91$.}
	\label{good_dsinr_91.imp}
\end{figure}

%\subsubsection*{Отображение результатов прогнозов}


\subsubsection{Выводы}
Эксперимент показал, что выбор недостаточно большого $r$ в гибридных моделях ведет к сильному ухудшению в точности прогнозирования.


\subsection{Влияние наличия шума во временном ряде}
\label{dsinenoise}
Поставим задачу сравнить влияние наличие шума во временном ряде на точность в прогнозировании обычными и гибридными методами, а также в прогнозировании методом SSA. Будем проводить эксперимент на рядах $\mathsf{X}_{620}$ и $\mathsf{Z}_{620}$. Так как очевидно, что результаты на ряде без шума, будут лучше, чем на ряде с шумом, то будем сравнивать общую динамику на графиках. 

\subsubsection{Прогноз по SSA}
Зададим следующую сетку параметров $L = \{ 13, 26, \ldots, 160 \}$, $r = \{ 8, 12, 16 \}$ для ряда обоих рядов. 

Посмотрим на результаты представленные в таблице \ref{dsinenoise_ssa_forecast}. Из таблицы видно, что для зашумленного ряда все лучшие результаты приходятся для $r=8$, также выбираются большие параметры $L$. 

\begin{table}[H]
	\caption{Лучшие пять результатов прогнозов, сделанные методом SSA}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\cline{1-4} \cline{6-9}
			$\mathsf{X}_{620}$ & RMSE  & L   & r  &  & $\mathsf{Z}_{620}$ & RMSE   & L   & r  \\ \cline{1-4} \cline{6-9} 
			1                  & 0.414 & 52  & 8  &  & 1                  & 11.591 & 130 & 8  \\ \cline{1-4} \cline{6-9} 
			2                  & 0.415 & 65  & 8  &  & 2                  & 11.605 & 117 & 8  \\ \cline{1-4} \cline{6-9} 
			3                  & 0.429 & 78  & 8  &  & 3                  & 11.682 & 91  & 8  \\ \cline{1-4} \cline{6-9} 
			4                  & 0.447 & 117 & 12 &  & 4                  & 11.688 & 143 & 8  \\ \cline{1-4} \cline{6-9} 
			5                  & 0.448 & 117 & 8  &  & 5                  & 11.736 & 78  & 8  \\ \cline{1-4} \cline{6-9} 
			25                 & 0.496 & 160 & 12 &  & 14                 & 12.446 & 160 & 12 \\ \cline{1-4} \cline{6-9} 
		\end{tabular}
	\end{center}
	\label{dsinenoise_ssa_forecast}
\end{table}

Далее для каждого ряда зафиксируем две пары параметров для которых продолжим исследование. Для ряда $\mathsf{X}_{620}$ зафиксируем аналитически верную пару $L = 160, \; r = 12$ и лучшую пару из таблицы \ref{dsineleninfl_ssa_forecast} $L = 52, \; r = 8$. Для ряда $\mathsf{Z}_{620}$ аналогично зафиксируем $L = 375, \; r = 12$ и $L = 130, \; r = 8$.

	
\subsubsection{Восстановление SSA}
	
\subsubsection{Сравнение обычных и гибридных методов}
	
\subsubsection{Проверка устойчивости}
	
\subsubsection{Отображение результатов прогнозов}
	
\subsubsection{Выводы}
	

\end{comment}

\clearpage

\section{Сумма двух синусов с красным шумом. Ряд с трудно отделимым сигналом}
\label{ser}
Рассмотрим следующий ряд $\mathsf{Z}_{650}$ (рис. \ref{ser_graph}), состоящий из элементов: $$ z_i = \big(\sin(2 \pi \dfrac{i}{6} ) + 2 \cdot \sin(2 \pi \dfrac{i}{12} ) \big) + \xi_i, $$ где $\xi_i = \xi_{i-1} + \sigma \varepsilon_{i-1}$, $\sigma = 1.2$, $\varepsilon_i \in \mathrm{N}(0, 1)$. 

Сигнал данного ряда идентичен сигналу ряда из раздела \ref{edsine}. Отличие состоит в шуме в этой главе он красный, что делает выделение сигнала сложной задачей, так как он сильно смешивается с шумом.  Это можно увидеть на периодограмме (рис. \ref{ser_pgram}). Из формулы видно, что ряд стоит из двух синусов с периодами $12$ и $6$, общий период  ряда $12$. В дальнейшем будем выбирать сетки для параметров $T$ и $L$ кратные $12$. Исходя из раздела \ref{edsine}, можно зафиксировать аналитически верными параметры $L = 175, \; r = 4$, но так как из-за красного шума сигнал сложно выделить, то не возможно данная пара будет не оптимальной. Далее в главе сравним разные параметры для метода SSA, выберем те которые дают хорошую точность, а также не сильную аппроксимацию ряд оценкой сигнала.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/ser/ser}}
	\caption{Ряд суммы синусов с красным шумом $\mathsf{Z}_{650}$.}
	\label{ser_graph}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/ser/ser_pgram}}
	\caption{Периодограмма ряда $\mathsf{Z}_{650}$.}
	\label{ser_pgram}
\end{figure}

\subsection{Сравнение прогнозов, полученных с помощью метода SSA, обычных и гибридных методов}
\label{serr}

Поставим задачу сравнить обычные, гибридные методы и метод SSA на ряде $\mathsf{Z}_{650}$. Проводить сравнение будем по методу, описанным в главе \ref{comp_tactic}. В ходе эксперимента хотим посмотреть как поведут себя гибридные методы, столкнувшись с рядом, в котором сложно корректно выделить сигнал. Будем проводить сравнение для аналитически верной пары $L = 175, \; r = 4$, а также для оптимальной пары, которую выделим на этапе сравнения параметров в методе SSA.

\subsubsection{Прогноз по SSA}
Зададим следующую сетку параметров $L = \{ 12, 24, \ldots, 175 \}$, $r = \{ 2, 4, 6, 8, 10, 12, 16 \}$. 

Посмотрим на результаты на рисунке \ref{serr_ssa_forecast}. На графике видно, что чем меньше $r$, тем выше ошибка. Для $r \geqslant 8$ разница между ошибками не такая большая, и результаты начинают смешиваться. Из графика видно, что $r = 2$ дает результаты сильно хуже, что говорит о сильном влияние шума. Также заметим, что ошибка для аналитическо верной пары растет с ростом $L$. 

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_comp}}
	\caption{Сумма синусов с красным шумом. Прогнозирование с помощью метода SSA.}
	\label{serr_ssa_forecast}
\end{figure}


%Ниже в таблице \ref{serr_ssa_forecast} представленны лучшие пять результатов и аналитически верная пара. Видно, что наилучший результат достигается для пары параметров $L = 36, \; r = 16$. Аналитически верная пара находится на $91$ месте ранжированного списка, ошибка для этой пары параметров довольно большая. Из таблицы видно, что лучшие результаты достигаются на маленьких $L$ и больших $r$. Такой выбор параметров приводит к высокой аппроксимации временного ряда оценкой сигнала ряда. 
%
%\begin{table}[H]
%	\caption{Лучшие пять результатов прогнозов с помощью метода SSA}
%	\begin{center}
%			\begin{tabular}{|c|c|c|c|}
%				\hline
%				$\mathsf{Z}_{650}$ & RMSE  & L   & r  \\ \hline
%				1                  & 1.611 & 36  & 16 \\ \hline
%				2                  & 1.614 & 24  & 10 \\ \hline
%				3                  & 1.653 & 24  & 12 \\ \hline
%				4                  & 1.663 & 96  & 16 \\ \hline
%				5                  & 1.667 & 24  & 8  \\ \hline
%				91                 & 2.371 & 175 & 4  \\ \hline
%			\end{tabular}
%	\end{center}
%	\label{ser_ssa_forecast}
%\end{table}
%
%Далее для каждого ряда зафиксируем две пары параметров для которых продолжим исследование.  Зафиксируем аналитически верную пару $L = 175, \; r = 4$ и лучшую пару из таблицы \ref{ser_ssa_forecast} $L = 36, \; r = 16$. 

\subsubsection{Восстановление SSA}

Посмотрим как SSA восстанавливает тренировочную выборку. Посмотрим на график зависимости ошибка восстановления тренировочной выборки от параметра $L$. Сетку для параметров оставим такую же. На графике \ref{serr_ssa_reconstruct} представлены результаты. Видно, что наилучшие результаты показывает пара $r = 2, \; L = 175$. Для других $r$ ситуация выглядит похоже. Для сравнения возьмем пару $r = 6, \; L = 175$ и аналитически верную пару $r = 4, \; L = 175$. Также возьмем две пары $r = 16, \; L = 36$ и $r = 14, \; L = 84$, как показывающие низкую ошибку на графике \ref{serr_ssa_forecast}. Для выбранных пар покажем как метод SSA восстанавливает сигнал. 

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_reconstruct}}
	\caption{Сумма синусов с красным шумом. Зависимость ошибки восстановления от параметра $T$.}
	\label{serr_ssa_reconstruct}
\end{figure}

Ниже на графиках представленны результаты. Видно, что лучше всего метод SSA восстанавливает сигнал с параметрами $L = 175, \; r = 2$ (рис. \ref{ser_ssa_LH_r2}). Для остальных пар наблюдается влияние шума. На рисунках \ref{ser_ssa_LH_r6}, \ref{ser_ssa_LH_r4},\ref{ser_ssa_L36_r16}, \ref{ser_ssa_L84_r14} можно наблюдать, что для параметров $L = 175$ и $r = \{4, 6\}$ влияние шума меньше, чем для пар $L = 36, \; r = 16$, $L = 84, \; r = 14$. На рисунке \ref{ser_ssa_r4_r6} можно заметить, что разницы в восстановленных сигналах для пар $L = 175 \; r = 4$ и $L = 175 \; r = 6$ не очень большая. Аналогичные выводы можно сделать для пар $L = 36, \; r = 16$, $L = 84, \; r = 14$ на рисунке \ref{ser_ssa_r14_r16}.

Далее в эксперименте рассмотрим три пары $L = 175, \; r = 2$, $L = 175, \; r = 4$ и $L = 84, \; r = 14$. Пара $L = 175, \; r = 2$ дает наилучшее восстановление ряда. Выбирая из пар $L = 36, \; r = 16$, $L = 84, \; r = 14$, взяли вторую, так как в теории она дает меньшую аппроксимацию оценкой сигнала ряда. Пару $L = 175, \; r = 4$ выбрали как аналитически верную.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_LH_r2}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 175, \; r = 2$.}
	\label{ser_ssa_LH_r2}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_LH_r6}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 175, \; r = 6$.}
	\label{ser_ssa_LH_r6}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_LH_r4}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 175, \; r = 4$.}
	\label{ser_ssa_LH_r4}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_L36_r16}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 36, \; r = 16$.}
	\label{ser_ssa_L36_r16}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_L84_r14}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 84, \; r = 14$.}
	\label{ser_ssa_L84_r14}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_r4_r6}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 175, \; r = 4$ и $L = 175, \; r = 6$.}
	\label{ser_ssa_r4_r6}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ssa_r16_r14}}
	\caption{Сумма синусов с красным шумом. Восстановление тренировочной выборки с помощью метода SSA. $L = 84, \; r = 14$ и $L = 36, \; r = 16$.}
	\label{ser_ssa_r14_r16}
\end{figure}



\subsubsection{Сравнение обычных и гибридных методов}

Пусть задана следующая сетка параметров: $T = \{12, 24, \ldots, 144 \}$, $h = \{10, 25, \ldots, 100 \}$. Для метода SSA в гибридных моделях возьмем пары параметров, выбранные заранее.

Для прогнозу по методу SSA зададим сетку, что была ранее: $L = \{ 12, 24, \ldots, 175 \}$, $r = \{ 2, 4, 6, 8, 10, 12, 16 \}$.

На графиках \ref{serr_r2}, \ref{serr_r2.h}, \ref{serr_r4}, \ref{serr_r4.h}, \ref{serr_r14}, \ref{serr_r14.h} представленны результаты сравнения по сетке параметров, заданной выше. На графиках видно, что метод SSA и гибридные методы сильно проигрывают обычных нейронным сетям. Это объяснимо тем, что ошибка считается относительно всего ряда. Возможно, гибридные методы предсказывают сигнал ряда, а он в свою очередь сильно отличен от временного ряда с шумом. Чтобы проверить это посмотрим на примеры предсказаний в следующем разделе.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r2}}
	\caption{Сумма синусов с красным шумом. Ошибки прогноза в зависимости от \linebreak параметра $T$. $L = 175, \; r = 2$.}
	\label{serr_r2}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r2.h}}
	\caption{Сумма синусов с красным шумом. Ошибки прогноза в зависимости от \linebreak параметра $h$. $L = 175, \; r = 2$.}
	\label{serr_r2.h}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r4}}
	\caption{Сумма синусов с красным шумом. Ошибки прогноза в зависимости от \linebreak параметра $T$. $L = 175, \; r = 4$.}
	\label{serr_r4}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r4.h}}
	\caption{Сумма синусов с красным шумом. Ошибки прогноза в зависимости от \linebreak параметра $h$. $L = 175, \; r = 4$.}
	\label{serr_r4.h}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r14}}
	\caption{Сумма синусов с красным шумом. Ошибки прогноза в зависимости от \linebreak параметра $T$. $L = 84, \; r = 14$.}
	\label{serr_r14}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r14.h}}
	\caption{Сумма синусов с красным шумом. Ошибки прогноза в зависимости от \linebreak параметра $h$. $L = 84, \; r = 14$.}
	\label{serr_r14.h}
\end{figure}

\subsubsection{Отображение прогнозов}

На графиках ниже представленны результаты прогнозирования методами (на графиках показано отклонение от сигнала ряда). Можем заметить как сильно обычные методы пытаются предсказывать шум. Для пары $L = 84, \; r = 14$ предсказание сигнала получилось не очень хорошее. Для пары $L = 175, \; r = 4$ получилось хорошее предсказание сигнала методов SSA-RNN и SSA-GRU. Для пары $L = 175, \; r = 2$ хорошее предсказание получилось для трех методов SSA-RNN, SSA-GRU и SSA-LSTM.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r2_res_ann}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для ANN и SSA-ANN. $L = 175, \; r = 2$}
	\label{serr_r2_res_ann}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r2_res_rnn}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для RNN и SSA-RNN. $L = 175, \; r = 2$}
	\label{serr_r2_res_rnn}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r2_res_gru}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для GRU и SSA-GRU. $L = 175, \; r = 2$}
	\label{serr_r2_res_gru}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r2_res_lstm}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для LSTM и SSA-LSTM. $L = 175, \; r = 2$}
	\label{serr_r2_res_lstm}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r4_res_ann}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для ANN и SSA-ANN. $L = 175, \; r = 4$}
	\label{serr_r4_res_ann}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r4_res_rnn}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для RNN и SSA-RNN. $L = 175, \; r = 4$}
	\label{serr_r4_res_rnn}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r4_res_gru}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для GRU и SSA-GRU. $L = 175, \; r = 4$}
	\label{serr_r4_res_gru}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r4_res_lstm}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для LSTM и SSA-LSTM. $L = 175, \; r = 4$}
	\label{serr_r4_res_lstm}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r14_res_ann}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для ANN и SSA-ANN. $L = 84, \; r = 14$}
	\label{serr_r14_res_ann}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r14_res_rnn}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для RNN и SSA-RNN. $L = 84, \; r = 14$}
	\label{serr_r14_res_rnn}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r14_res_gru}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для GRU и SSA-GRU. $L = 84, \; r = 14$}
	\label{serr_r14_res_gru}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/serr_r14_res_lstm}}
	\caption{Сумма синусов с красным шумом. Прогноз результатов для LSTM и SSA-LSTM. $L = 84, \; r = 14$}
	\label{serr_r14_res_lstm}
\end{figure}

\subsubsection{Проверка устойчивости}

Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{12, 84\}$. Сетка для параметр $h$ останется прежней. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. Полученные результаты отображены на рисунках ниже. На них подтверждается, выводы сделанные ранее. Заключаем, что полученные результаты устойчивые. 

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/loopr2.12}}
	\caption{Сумма синусов с красным шумом. Проверка устойчивости. $r = 2, \; L = 175$. $T = 12$.}
	\label{serrloopsr2.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/loopr2.84}}
	\caption{Сумма синусов с красным шумом. Проверка устойчивости. $r = 2, \; L = 175$. $T = 84$.}
	\label{serrloopsr2.84}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/loopr4.12}}
	\caption{Сумма синусов с красным шумом. Проверка устойчивости. $r = 4, \; L = 175$. $T = 12$}
	\label{serrloopsr4.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/loopr4.84}}
	\caption{Сумма синусов с красным шумом. Проверка устойчивости. $r = 4, \; L = 175$. $T = 84$.}
	\label{serrloopsr4.84}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/loopr14.12}}
	\caption{Сумма синусов с красным шумом. Проверка устойчивости. $r = 14, \; L = 84$. $T = 12$}
	\label{serrloopsr14.12}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/loopr14.84}}
	\caption{Сумма синусов с красным шумом. Проверка устойчивости. $r = 14, \; L = 84$. $T = 84$.}
	\label{serrloopsr14.84}
\end{figure}

\subsubsection{Выводы}



%\subsubsection{Проверка устойчивости}
%
%Посмотрим на рисунки \ref{ser13}, \ref{ser78}, \ref{ser13.imp}, \ref{ser78.imp}. На графиках видно, что полученные результаты повторяют картину из предыдущих графиков. Можем заключить, что результаты устойчивы.
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/13}}
%	\caption{Проверка устойчивости. Пара $L = 175, \; r = 8$. $T = 13$.}
%	\label{ser13}
%\end{figure}
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/78}}
%	\caption{Проверка устойчивости. Пара $L = 175, \; r = 8$. $T = 78$.}
%	\label{ser78}
%\end{figure}
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/13.imp}}
%	\caption{Проверка устойчивости. Пара $L = 26, \; r = 8$. $T = 13$.}
%	\label{ser13.imp}
%\end{figure}
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/78.imp}}
%	\caption{Проверка устойчивости. Пара $L = 26, \; r = 8$. $T = 78$.}
%	\label{ser78.imp}
%\end{figure}
%
%\subsubsection{Отображение результатов прогнозов}
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/ann}}
%	%\caption{Проверка устойчивости. Пара $L = 175, \; r = 8$. $T = 13$.}
%	%\label{ser13}
%\end{figure}
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/rnn}}
%	%\caption{Проверка устойчивости. Пара $L = 175, \; r = 8$. $T = 78$.}
%	%\label{ser78}
%\end{figure}
%
%\begin{figure}[H]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/gru}}
%	%\caption{Проверка устойчивости. Пара $L = 26, \; r = 8$. $T = 13$.}
%	%\label{ser13.imp}
%\end{figure}
%
%\begin{figure}[h]
%	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/ser/lstm}}
%	%\caption{Проверка устойчивости. Пара $L = 26, \; r = 8$. $T = 78$.}
%	%\label{ser78.imp}
%\end{figure}
%
%\subsubsection{Выводы}
%Эксперимент показал, что если метод SSA не может корректно выделить сигнал, то препроцессинг SSA в гибридных моделях приводит к сильному ухудшению результатов. Что делает использование препроцессинга SSA в задачах с таким типом рядов бессмысленным. Во время эксперимента была проверена устойчивость результатов.

%==================================================================
%==================================================================
%==================================================================


\chapter{Реальные данные}
\section{Среднемесячные осадки в Индии}
\label{indian_rain}

Рассмотрим следующий ряд $\mathsf{Z}_{1500}$ (рис. \ref{indian_rain_img}) взятый из статьи \cite{ar1}. Данные <<Indian Rain>> показывают среднемесячное количество осадков в Индии.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/indian_rain/indian_rain}}
	\caption{Данные <<Indian Rain>>.}
	\label{indian_rain_img}
\end{figure}

В экспериментах будем разбивать ряд $\mathsf{Z}_{1500}$ на тренировочную, валидационную, тестовую выборки по $750, 500, 250$ точек соответственно. 

На рисунке \ref{indian_rain_pgram} можно увидеть периодограмму ряда. Видно, что ряд имеет три периодики и трендовую составляющую. Исходя из рис. \ref{indian_rain_img} это константный тренд. Ввиду этого, будем считать параметры $r=7$ и $L = 375$ аналитически верными для метода SSA и гибридных методов. Так как ранг ряда скорее всего равен $7$, а $L = 375$ удовлетворяет асимптотической разделимости. Также, так как это данные по месяцам, то период ряда кратен $12$. Далее в экспериментах будем перебирать параметры $T$ и $L$ по сетке с шагом кратным $12$. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/indian_rain/indian_rain_pgram}}
	\caption{Периодограмма ряда <<Indian Rain>>.}
	\label{indian_rain_pgram}
\end{figure}

\subsection{Сравнение прогнозов, полученных с помощью метода SSA, обычных и гибридных методов}
Сравним метод SSA, обычные и гибридные методы по способу, описанным в \linebreak главе \ref{comp_tactic}. 

\subsubsection{Прогноз по SSA}
Сравним точность прогнозирования методом SSA при разных параметрах. Зададим следующую сетку параметров $L = \{12, 24, \ldots, 375\}$, $r = \{5, 7, 9, 11\}$. Посмотрим на результаты на рисунке \ref{rain_ssa_forecast}. На графике видно, что наилучшие результаты достигаются при $r = 11$. Разницы в параметрах $L$ нет. Далее посмотрим на две пары параметров $r = 11, \; L = 375$ и $r = 7, \; L = 375$.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/ssa_comp}}
	\caption{Данные <<Indian Rain>>. Прогнозирование с помощью метода SSA.}
	\label{rain_ssa_forecast}
\end{figure}

\subsubsection{Восстановление SSA}

Посмотрим, как метод SSA восстанавливает тренировочную выборку для выбранных пар на рис. \ref{indian_rec_r7}, \ref{indian_rec_r11}, \ref{indian_rec_rboth}. На графиках видно, что метод весьма хорошо выделил сигнал. Для $r = 7$ пики в оценке одинаковые. Для $r=11$ пики у оценки немного скачут, возможно это эффект шума, попавшего в ряд. Дальше будем использовать параметр $r = 7$ в методе SSA и гибридных параметрах, параметр $L = 375$ в гибридных моделях.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/indian_rec_r7}}
	\caption{Данные <<Indian Rain>>. Восстановление тренировочной выборки с помощью метода SSA. $r = 7, \; L = 375$.}
	\label{indian_rec_r7}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/indian_rec_r11}}
	\caption{Данные <<Indian Rain>>. Восстановление тренировочной выборки с помощью метода SSA. $r = 11, \; L = 375$.}
	\label{indian_rec_r11}
\end{figure}

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/indian_rec_rboth}}
	\caption{Данные <<Indian Rain>>. Восстановление тренировочной выборки с помощью метода SSA. Обе пары.}
	\label{indian_rec_rboth}
\end{figure}

\subsubsection{Сравнение методов}

Пусть задана следующая сетка параметров: $T = \{12, 48, \ldots, 480 \}$, $h = \{10, 25, \ldots, 100 \}$.

Для метода SSA зафиксируем параметр $r = 7$, а параметр $L$ будем перебирать по сетке $\{12, 24, \ldots, 480 \}$.

На рис. \ref{first_comp}, \ref{first_comp.h} видно, что гибридные методы показывают наилучшие результаты. Прогноз с помощью метода SSA находится посередине. На графике \ref{first_comp} особено виден отрыв для $T = 12$ (левый край графика). Также видно, что у гибридных методов почти нет зависимости ошибки от выбора параметра $T$.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/first_comp}}
	\caption{Данные <<Indian Rain>>. Ошибки прогноза в зависимости от параметра $T$.}
	\label{first_comp}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/first_comp.h}}
	\caption{Данные <<Indian Rain>>. Ошибки прогноза в зависимости от параметра $h$.}
	\label{first_comp.h}
\end{figure}

\subsubsection{Отображение прогнозов}

На графиках ниже видно, что прогнозирование обычными и гибридными методами практически одинаковое.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/rain_res_ann}}
	\caption{Данные <<Indian Rain>>. Прогноз для ANN и SSA-ANN.}
	\label{rain_res_ann}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/rain_res_rnn}}
	\caption{Данные <<Indian Rain>>. Прогноз для RNN и SSA-RNN.}
	\label{rain_res_rnn}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/rain_res_gru}}
	\caption{Данные <<Indian Rain>>. Прогноз для GRU и SSA-GRU.}
	\label{rain_res_gru}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/rain_res_lstm}}
	\caption{Данные <<Indian Rain>>. Прогноз для LSTM и SSA-LSTM.}
	\label{rain_res_lstm}
\end{figure}

\subsubsection{Проверка устойчивости}

Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{12, 156\}$. Сетка для параметр $h$ останется прежней. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. Полученные результаты отображены на рис. \ref{2.12}, \ref{2.156}. 

На рисунках видно, что полученные ранее результаты не случайны. Гибридные модели показывают лучшие результаты, особенно хорошо это видно для $T = 12$.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/2.12}}
	\caption{Данные <<Indian Rain>>. Проверка устойчивости. $T = 12$.}
	\label{2.12}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/indian_rain/2.156}}
	\caption{Данные <<Indian Rain>>. Проверка устойчивости. $T = 156$.}
	\label{2.156}
\end{figure}

\subsubsection{Выводы}
Эксперимент показал, что для данных <<Indian rain>> использование гибридных методов приводит к хорошему приросту в точности. Также гибридные методы снижает зависимость ошибки от выбора параметров модели, что позволят выбрать менее сложную модель, а также увеличивает количество пар <<признаки -- предсказываемые значения>>.

Успех гибридных методов объясним тем, что ряд <<Indian Rain>> имеет простой сигнал конечного ранга. Ряд имеет хорошую длину, что обеспечивает лучшее выделение сигнала и большее количество пар <<признаки -- предсказываемые значения>>, это положительно сказывается на точности предсказаний гибридных моделей. Также в ряде есть шум, что делает использование препроцессинга SSA логичным.

\clearpage

\section{Данные Earth Orientation Parameters (EOP)}
\label{eop}

Рассмотрим следующий ряд $\mathsf{Z}_{717}$ (рис. \ref{x_pole}). Временной ряд отображает координату по оси абсцисс земного полюса.

\begin{figure}[H]
	\center{\includegraphics[width=1\linewidth]{imgs/ts/eop/x_pole}}
	\caption{Временной ряд <<x pole>>.}
	\label{x_pole}
\end{figure}

Вычтем тренд из $\mathsf{Z}_{717}$ (рис. \ref{x_pole_no_trend}). Будем считать, что правая часть ряда не похожа на остальной временной ряд, поэтому удалим ее, чтобы избежать искаженных результатов. Эта операция сократит размер ряда $\mathsf{Z}_{717}$ до $620$ точек. Обозначим ряд слева от вертикальной черты $\mathsf{Z}_{620}$. Далее все эксперименты в разделе \ref{eop} проводятся на $\mathsf{Z}_{620}$. Также, так как это данные по месяцам, то период ряда кратен $12$. Но исходя из периодограммы, видно, что с пиком в $1$ есть еще пик, который вдвое больше. Считаем, что это пик в $14$, тогда общий период ряда равен $13$. Далее в экспериментах будем перебирать параметры $T$ и $L$ по сетке с шагом кратным $13$.

\begin{figure}[H]
	\center{\includegraphics[width=1\linewidth]{imgs/ts/eop/x_pole.notrend}}
	\caption{Временной ряд <<x pole>> без тренда.}
	\label{x_pole_no_trend}
\end{figure}

В экспериментах будем разбивать ряд $\mathsf{Z}_{620}$ на тренировочную, валидационную, тестовую выборки по $320, 150, 150$ точек соответственно. 

Посмотрим на периодограмму ряда $\mathsf{Z}_{620}$ на рис. \ref{x_pole_pgram}. На графике видно две близкие периодики, которые смешались. Это говорит, что у ряда сложный сигнал, который будет трудно выделить корректно. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/ts/eop/x_pole_pgram}}
	\caption{Периодограмма ряда <<x pole>> без тренда.}
	\label{x_pole_pgram}
\end{figure}

\subsection{Сравнение прогнозов, полученных с помощью метода SSA, обычных и гибридных методов}

Сравним метод SSA, обычные и гибридные методы по способу, описанным в \linebreak главе \ref{comp_tactic}. В разделе <<Прогноз по SSA>> подберем оптимальные параметры для метода SSA и гибридных моделей.

\subsubsection{Прогноз по SSA}
Сравним точность прогнозирования методом SSA при разных параметрах. Зададим следующую сетку параметров $L = \{13, 26, \ldots, 160\}$, $r = \{6, 8, 12, 16, 18\}$. 

Посмотрим на результаты на рисунке \ref{eop_ssa_forecast}. На графике видно, что с ростом $L$ растет и ошибка. Наилучшая точность достигается при $r = \{12, 16, 18\}$ и маленьком $T$. Такие результаты могут свидетельствовать о том, что сигнал ряда $\mathsf{Z}_{620}$ не конечного ранга. Использование маленького $T$ и большого $r$ в препроцессинге SSA приведет к сильной аппроксимации оценкой сигнала ряда, что делает использование SSA нецелесообразным. Возьмем пару параметров по середине: $L = 78, \; r = 18$, далее будем использовать эту пару в гибридных моделях. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/ssa_comp}}
	\caption{Ряд <<x pole>> без тренда. Прогнозирование с помощью метода SSA.}
	\label{eop_ssa_forecast}
\end{figure}

\subsubsection{Восстановление SSA}
Посмотрим, как метод SSA восстанавливает тренировочную выборку для выбранных пар на рис. \ref{eop_rec}. На графике видно, что метод неплохо выделил сигнал. Видно, что оценка сигнала сильно аппроксимирует временной ряд, кроме нескольких мест. 

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/eop_rec}}
	\caption{Ряд <<x pole>> без тренда. Восстановление тренировочной выборки с помощью метода SSA. $r = 18, \; L = 78$}
	\label{eop_rec}
\end{figure}

\subsubsection{Сравнение методов}

Пусть задана следующая сетка параметров: $T = \{13, 42, \ldots, 143 \}$, $h = \{10, 25, \ldots, 100 \}$.

Для метода SSA зафиксируем параметр $r = 12$, а параметр $L$ будем перебирать по сетке $\{13, 26, \ldots, 130 \}$.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/x_pole.comp.T}}
	\caption{Ряд <<x pole>> без тренда. Ошибки прогноза в зависимости от параметра $T$.}
	\label{x_pole_comp}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/x_pole.comp.T.h}}
	\caption{Ряд <<x pole>> без тренда. Ошибки прогноза в зависимости от параметра $h$.}
	\label{x_pole_comp.h}
\end{figure}

Посмотрим на результаты на рис. \ref{x_pole_comp}. На графике можно видеть, что наилучший результат показывает метод SSA с параметром $L = 26$. Видно гибридные методы сильно уступают в точности обычным методам. Методы GRU и RNN показывают хорошую точность. 

\subsubsection{Отображение прогнозов}

На графиках ниже видно, что прогнозирование сильно расходится в месте так называемого <<перехода>>. Ближе всего в этом месте прогнозирует метод RNN, что видно на графике \ref{xpole_res_rnn}. Также есть сильное расхождение с сигналом ряда в конце прогноза (правая часть графиков). Для всех гибридных методов ошибка в этих местах больше, чем для обычных. В остальных местах значительной разницы между прогнозами не замечено. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/xpole_res_ann}}
	\caption{Ряд <<x pole>> без тренда. Прогноз для ANN и SSA-ANN.}
	\label{xpole_res_ann}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/xpole_res_rnn}}
	\caption{Ряд <<x pole>> без тренда. Прогноз для RNN и SSA-RNN.}
	\label{xpole_res_rnn}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/xpole_res_gru}}
	\caption{Ряд <<x pole>> без тренда. Прогноз для GRU и SSA-GRU.}
	\label{xpole_res_gru}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/xpole_res_lstm}}
	\caption{Ряд <<x pole>> без тренда. Прогноз для LSTM и SSA-LSTM.}
	\label{xpole_res_lstm}
\end{figure}

\subsubsection{Проверка устойчивости}

Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{13, 91\}$. Сетка для параметр $h$ останется прежней. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. Полученные результаты отображены на рис. \ref{loop13}, \ref{loop91}. На рисунках видно, что полученные ранее результаты не случайны.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/loop13}}
	\caption{Ряд <<x pole>> без тренда. Проверка устойчивости. $T = 13$.}
	\label{loop13}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/eop/loop91}}
	\caption{Ряд <<x pole>> без тренда. Проверка устойчивости. $T = 91$.}
	\label{loop91}
\end{figure}

\subsubsection{Выводы}

В ходе сравнение обычных, гибридных методов и метода SSA наилучшие результаты показал метод SSA с параметрами $L = 26, r = 12$, что равносильно использование линейной рекуррентной формулы для обычного ряда. Использование таких параметров в гибридных моделях нецелесообразно, так как оценка сигнала слишком сильно аппроксимирует ряд. Гибридные методы показали результаты хуже, чем обычные методы. 

Причина таких результатов может быть в том, что сигнал ряда скорее всего неконечного ранга, что делает корректное выделение сигнала затруднительным. Ошибки на картинках с прогнозами регулярно происходят в 2 местах, что говорит о том, что SSA некорректно выделяет сигнал, а это приводит гибридные методы к плохим результатам. Также проблема может возникнуть в небольшой длине ряда, что может плохо сказаться на выделении сигнала методом SSA и обучении нейронных сетей (дефицит обучающих пар). Шум ряда маленький, что ставит под сомнение использование препроцессинга SSA. Как было показано в эксперименте \ref{edsinr_lownoise} в случае маленького шума, логично выбрать параметр $r$ оптимальный или больше. В этом случае мы не можем выделить оптимальный параметр аналитический, что заставляет искать параметр $r$ перебором, что в свою очередь сподвигает выбрать параметр $r$ побольше. Недостаточно большой параметр $r$ с большой вероятностью приведет к некорректному выделению сигнала, так как все компоненты сложного сигнала могут не попасть в оценку. Большой параметр $r$ должен захватить весь сигнал, но ввиду того что сигнал ряда неконечного ранга, параметр $L$ должен быть выбран относительно малым. Большой параметр $r$ и малый параметр $L$ приводят нас к сильной аппроксимации оценкой сигнала ряда, что делает препроцессинг SSA бессмысленным. 

\clearpage

\section{Погода}
\label{weather}

Рассмотрим следующий ряд $\mathsf{Z}_{828}$ (рис. \ref{weather_graph}). Данный ряд отображает одну из характеристик погоду в городе Санкт-Петербурге.

\begin{figure}[H]
	\center{\includegraphics[width=1\linewidth]{imgs/ts/weather/weather4}}
	\caption{Временной ряд характеристики погоды в Санкт-Петербурге.}
	\label{weather_graph}
\end{figure}

В экспериментах будем разбивать ряд $\mathsf{Z}_{828}$ на тренировочную, валидационную, тестовую выборки по $528, 150, 150$ точек соответственно. 

На рис. \ref{weather_pgram} представлена периодограмма ряда $\mathsf{Z}_{828}$. Из нее видно, что у ряда есть тренд и две периодики. Одна периодика слабовыраженная и может смешаться с шумом. Ввиду этого, будем считать параметры $r=5$ и $L = 264$ аналитически верными для метода SSA и гибридных методов. Так как ранг ряда скорее всего равен $5$, а $L = 264$ удовлетворяет асимптотической разделимости. Также, так как это данные по месяцам, то период ряда кратен $12$. Далее в экспериментах будем перебирать параметры $T$ и $L$ по сетке с шагом кратным $12$.

\begin{figure}[H]
	\center{\includegraphics[width=1\linewidth]{imgs/ts/weather/weather4_pgram}}
	\caption{Периодограмма ряда характеристики погоды.}
	\label{weather_pgram}
\end{figure}

\subsection{Сравнение прогнозов, полученных с помощью метода SSA, обычных и гибридных методов}

Сравним метод SSA, обычные и гибридные методы по способу, описанным в \linebreak главе \ref{comp_tactic}. 

\subsubsection{Прогноз по SSA}

Сравним точность прогнозирования методом SSA при разных параметрах. Зададим следующую сетку параметров $L = \{12, 24, \ldots, 264\}$, $r = \{3, 5, 7, 9\}$. Посмотрим на результаты на рисунке \ref{weather_ssa_forecast}. На графике видно, что наилучшие результаты достигаются при $r = 5$. Нету сильной разницы в точности при $r=5$, поэтому дальше будем рассматривать пару $r = 5, \; L = 264$.


\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/ssa_comp}}
	\caption{Погода в Санкт-Петербурге. Прогнозирование с помощью метода SSA.}
	\label{weather_ssa_forecast}
\end{figure}

\subsubsection{Восстановление SSA}

Посмотрим, как метод SSA восстанавливает тренировочную выборку для выбранных пар на рис. \ref{weather_rec}. На графике видно, что метод неплохо выделил сигнал и оценка сигнала сильно не аппроксимирует временной ряд. 

\begin{figure}[H]
	\captionsetup{justification=centering}
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather_rec}}
	\caption{Погода в Санкт-Петербурге. Восстановление тренировочной выборки с помощью метода SSA. $r = 5, \; L = 264$}
	\label{weather_rec}
\end{figure}

\subsubsection{Сравнение методов}

Пусть задана следующая сетка параметров: $T = \{12, 48, \ldots, 120 \}$, $h = \{10, 25, \ldots, 100 \}$. Параметры для SSA в гибридных методах выберем $L = 264, \; r = 5$. Для метода SSA зафиксируем параметр $r = 5$, а параметр $L$ будем перебирать по сетке $\{12, 24, \ldots, 264 \}$.

Посмотрим на результаты на рис. \ref{weather_comp}, \ref{weather_comp.h}, видно, что лучшие результаты показывает метод SSA. Сложно оценить, какие методы показали себя лучше. На графике все результаты перемешаны, но кривые гибридных методов лежат немного ниже, чем обычных. Тем не менее, в некоторых частных случая гибридные методы показывают хорошую точность. Например SSA-ANN на рис. \ref{weather_comp} и SSA-ANN, SSA-RNN на рис. \ref{weather_comp.h}. Такие результаты могли получиться из-за маленького количества данных. А также возможно малого размера валидационной выборки, что ограничивает выбор параметра $T$.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather4.comp}}
	\caption{Погода в Санкт-Петербурге. Ошибки прогноза в зависимости от параметра $T$.}
	\label{weather_comp}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather4.comp.h}}
	\caption{Погода в Санкт-Петербурге. Ошибки прогноза в зависимости от параметра $h$.}
	\label{weather_comp.h}
\end{figure}

\subsubsection{Отображение прогнозов}

На графиках ниже видно, что прогнозирование обычными и гибридными методами похожее. Но заметно, что прогноз гибридных методов более стабилен. Эффект этого объясняется препроцессингом SSA. Видно, что прогнозы обычных методов, не использующие препроцессинг, более подверженны влиянию шума в ряде.

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather_res_ann}}
	\caption{Погода в Санкт-Петербурге. Прогноз для ANN и SSA-ANN.}
	\label{weather_res_ann}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather_res_rnn}}
	\caption{Погода в Санкт-Петербурге. Прогноз для RNN и SSA-RNN.}
	\label{weather_res_rnn}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather_res_gru}}
	\caption{Погода в Санкт-Петербурге. Прогноз для GRU и SSA-GRU.}
	\label{weather_res_gru}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/weather_res_lstm}}
	\caption{Погода в Санкт-Петербурге. Прогноз для LSTM и SSA-LSTM.}
	\label{weather_res_lstm}
\end{figure}

\subsubsection{Проверка устойчивости}
Чтобы исключить случайность в полученных результатах, проведем сравнение для разных начальных весов методов. Зафиксируем новую сетку для параметра $T = \{12, 84\}$. Сетка для параметр $h$ останется прежней. Будем получать каждый результат по 7 раз, инициализируя метод с новыми весами. Полученные результаты отображены на рисунках ниже. На них подтверждается, выводы сделанные ранее. Заключаем, что полученные результаты устойчивые. 

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/loops.12}}
	\caption{Погода в Санкт-Петербурге. Проверка устойчивости. $T = 12$.}
	\label{weather.loops.12}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.9\linewidth]{imgs/comp/weather/loops.84}}
	\caption{Погода в Санкт-Петербурге. Проверка устойчивости. $T = 84$.}
	\label{weather.loops.156}
\end{figure}

\subsubsection{Выводы}
Сравнение показало, что гибридные методы не дают сильной прироста в точности на данных погоды. Но все же в некоторых частных случаях получается достичь преимущества над обычными методами. 

Данный ряд похож на ряд <<Indian Rain>>, но с сигналом немного сложнее. Сигнал ряда погоды конечного ранга, в ряде присутствует шум. Аналитические выбранные параметры для метода SSA совпадают с выбранными перебором. Ряды погоды и <<Indian Rain>> очень похожи, но получить успех <<Indian Rain>> не получилось. Единственное их различие это длина ряда. Что приводит к выводу, что из-за малого количества данных, нейронные сети не могут уловить зависимость в данных и получить общее представление о данных, хоть и метод SSA корректно выделил сигнал.

\chapter*{Заключение}
\addcontentsline{toc}{chapter}{Заключение}
В работе был рассмотрена методика исследования сравнения методов, которая показалась весьма успешной. На данных Indian Rain удалось продемонстрировать успешное применение гибридных методов. Результаты на данных EOP и погоды в Санкт-Петербурге показали, что использование гибридных методов не всегда приводит к улучшению результата. 

Полученные результаты показали, что для использование препроцессинга SSA важно наличие трех факторов:

\begin{enumerate}
	\item Наличие в ряде шума.
	\item Метод SSA может корректно выделить сигнал (получить оценку сигнала).
	\item Длина ряда.
\end{enumerate}

По первому пункту, цель препроцессинга SSA в задачи прогнозирования временных рядов заключается в очистке ряда от шума, так если первый пункт не выполнен, не понятно зачем использовать препроцессинг SSA. Если шум в ряде маленький, то пепроцессинг SSA все еще можно использовать, если SSA может корректно получить оценку сигнала. В этом случае лучше выбрать параметр $r$ оптимальный или больше, чтобы точно захватить сигнал. Даже если шум попадет в оценку, если он небольшой, то его влияние будет незначительным. В случае значительного шума, стоит выбрать параметр $r$ оптимальный или меньше, чтобы шум не искажал оценку сигнала.

По второму пункту, если метод SSA не может корректно выделить сигнал, это приведет использование гибридных методов к некачественным прогнозам. В этом случае стоит отказаться от использовании препроцессинга SSA.

Немаловажную роль играет и длина ряда, так на достаточно длинных рядах удается лучше выделить сигнал методом SSA. Также параметры нейронных сетей лучше оптимизируются во время процесса обучения, что положительно сказываются на точности предсказании.

Так на основе изложенных выше информации, можно сказать, что использование препроцессинга на данных EOP является бессмысленным. Данные <<Indian Rain>> в свою очередь, являются идеальным экземпляром для использовании гибридных методов. Данные погоды в Санкт-Петербурге можно рассматривать, как данные <<Indian Rain>>, но меньшей длины, что подчеркивает важность длины временных рядов.

\bibliographystyle{gost2008}
\bibliography{library.bib}
 
\end{document}

