\documentclass[specialist,
               substylefile = spbu_report.rtx,
               subf,href,colorlinks=true, 12pt]{disser}

\usepackage[a4paper,
            mag=1000, includefoot,
            left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\ifpdf\usepackage{epstopdf}\fi
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{bm}
\usepackage{float}


% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% Использовать полужирное начертание для векторов
\let\vec=\mathbf

% Включать подсекции в оглавление
\setcounter{tocdepth}{2}

\graphicspath{{fig/}}

%----------------------------------------------------------------
\begin{document}

%
% Титульный лист на русском языке
%
% Название организации
\institution{%
    Санкт-Петербургский государственный университет\\
    Прикладная математика и информатика
}

\title{Учебная практика 2 (проектно-технологическая)}

% Тема
\topic{Использование метода SSA в машинном обучении для прогноза временных рядов}

% Автор
\author{Ежов Федор Валерьевич}
\group{группа 20.М03-мм}
    
% Научный руководитель
\sa       {Голяндина Нина Эдуардовна\\%
           Кафедра Статистического Моделирования}
\sastatus {к.физ.-мат.н., доцент}

% Город и год
\city{Санкт-Петербург}
\date{\number2021}

\maketitle

\tableofcontents

\chapter*{Введение}
\addcontentsline{toc}{chapter}{Введение}

В предыдущем семестре было показано, что вопреки утверждению из статьи \cite{ar1}, метод SSA в качестве препроцессинга улучшает точность прогноза методом ANN (полносвязная нейронная сеть с одним скрытым слоем). При этом использовались как данные из статьи, так и похожий по структуре модельный пример. В качестве модельного ряда для анализа был использовать зашумленный синусоидальный сигнал (исходный ряд осадков в Индии был похож на зашумленную сумму гармонических рядов). Анализ реальных данных и анализ модельного ряда приводил к схожим выводам, в связи с чем возникла идея при анализе реальных данных сначала строить их модель и потом исследовать методы на модели. Модель удобна тем, что в ней можно менять параметры, можно проверять устойчивости результатов, и пр.

В этом семестре была поставлена задача:

\begin{enumerate}
\item Рассмотреть большее количество видов NN
\item Рассмотреть новые реальные данные и построить для них модель
\item Выработать методику выбора метода ML, его параметров и сравнения методов между собой, включая гибридный варианты с SSA препроцессингом.
\end{enumerate}

По пункту 1 к ANN были добавлены рекуррентные методы: RNN, GRU и LSTM.
По 2 пункту были рассмотрены новые данные для параметров вращения Земли (EOP). Они оказались гораздо сложнее для анализа. Так как после вычитания тренда данные похожи на модулированную сумму гармоник с близкими частотами, было решено начать с исследования модельного примера такого типа.

Таким образом, образовалось два модельных примера, зашумленный чистый синус с периодом 12 (данные сезонные) и зашумленная модулированная сумма гармоник с близкими периодами 12 и 14.

При сравнении методов машинного обучения между собой возникает проблема неизвестной точности оценок величин ошибок. Так как в рассматриваемых методах ML везде есть полносвязный скрытый слой, то оказался очень удачным график зависимости ошибок на валидационных данных от числа нейронов в этом слое. Во-первых, по нему можно определить, какое число нейронов лучше брать. Но, главное, он отражает точность оценок, так как содержит случайные колебания.

Общая схема исследования на модельных примерах получилась такая:
\begin{itemize}
\item По графикам зависимости ошибок от числа нейронов выбор одновременно разумно-оптимального числа нейронов и размера входных данных.

\item Для выбранного числа нейронов и размера входных данных подбирать параметры метода SSA, используемого для препроцессинга. При этом варианты числа компонент для оценки сигнала методом SSA выбираются либо на основе теории метода SSA, либо на визуальном анализе результатов SSA разложения.
\end{itemize}

Структура работы следующая. В главе 1 описан алгоритм препроцессинга Singular Spectrum Analysis (SSA). В главе 2 описана основные методы и схемы, применяемые в экспериментах в области ML, описанных в главах 4 и 5. В главе 3 подробно описаны все методы прогнозирования, используемые в работе. В главе 4 описаны эксперименты на синтетических данных. В главе 5 описаны эксперименты на реальных данных.


\chapter{Алгоритм Singular Spectrum Analysis}

Метод SSA используется для разложение исходного ряда в сумму рядов, которые легко интерпретировать и понять их поведение. Обычно исходный ряд раскладывается в сумму трех рядов: тренд --- медленно меняющаяся компонента, сезонность --- циклическая компонента с фиксированным периодом и шум. Информацию про базовый алгоритм SSA и связанные с методом фундаментальные понятия можно найти в книге <<Analysis of time series structure: SSA and related techniques>> \cite{SSA}.

Алгоритм SSA состоит из четырех этапов:
\begin{enumerate}
	\item Построение траекторной матрицы (Вложение).
	\item SVD.
	\item Группировка первых r собственных троек.
	\item Диагональное усреднение.
\end{enumerate}

Рассмотрим каждый этап подробнее. \\
Пусть $\mathsf{X}_N = (x_1, \cdots, x_{N})$ --- временной ряд, где $N > 3$. Также будем предполагать, что найдется хоть одно $x_i \neq 0$, то есть ряд не нулевой. Обычно считается, что $x_i = f(i\Delta)$ для некоторой функции $f(t)$, где $t$ --- время, а $\Delta$ --- некоторый временной интервал. \\

\section{Этап 1. Построение траекторной матрицы (Вложение)}
Выберем целое $L$ --- длина окна, такое что $1 < L < N$. Тогда $K = N - L - 1$. 
Построим вектора 
$X_i = (x_{i} , \dots, x_{i+L-1})^T$, для $1 \leqslant i \leqslant K$.
Составим из векторов $X_i$ траекторную матрицу: 

$$\mathbf{X} = [X_1 : \cdots : X_K] = 
\begin{pmatrix}
	x_1& x_2& x_3& \cdots& x_{K} \\
	x_2& x_3& x_4& \cdots& x_{K+1} \\
	x_3& x_4& x_5& \cdots& x_{K+2} \\
	\vdots& \vdots& \vdots& \ddots& \vdots \\
	x_{L}& x_{L+1}& x_{L+2}& \cdots& x_{N} 
\end{pmatrix} 
.$$

Получили матрицу $\mathbf{X}$ размерностью $L \times K$, составленную из пересекающихся частей  исходного временного ряда. Можно заметить, что на побочных диагоналях стоят одинаковые числа, такая матрица называется ганкелевой. Существует взаимно-однозначное соответствие между ганкелевыми матрицами $L \times K$ и рядами длиной $N = L + K - 1$.

Операцию получения из ряда $\mathsf{X}_N$ траекторную матрицу $\mathbf{X}$ обозначим:
$$ \mathbf{X} = \mathcal{T}_L(\mathsf{X}_N), $$
соответственно обратная операция будет обозначаться: $\mathcal{T}^{-1}$.

\section{Этап 2. Singular Value Decomposition (SVD)}
На данном этапе применяется метод SVD к траекторной матрице $\mathbf{X}$. Пусть $ \mathbf{S}  = \mathbf{X}\mathbf{X}^T$ и $\lambda_1 > \dotsc > \lambda_L$ --- собственные числа матрицы $\mathbf{S}$, $U_1, \dotsc, U_L$ --- ортонормированная система базисных векторов, соответствующих собственным числам. Обозначим $V_i = \dfrac{X^T U_i}{\sqrt \lambda_i}$ и $d = max\{i : \lambda_i > 0\}$. Тогда сингулярное разложение матрицы $\mathbf{X}$ запишется следующим образом:

$$\mathbf{X} = \mathbf{X}_1 + \dotsc + \mathbf{X}_d \text{, где } \mathbf{X}_i = \sqrt \lambda_i U V^T, $$

Набор $(\sqrt \lambda_i, U_i, V^T_i)$ будем называть i-й собственной тройкой.

\section{Этап 3. Группировка первых r собственных троек}
На этапе группировки из всех значений $\{1 \dotsc d\}$ берутся первые $r$. Пусть, $I = \{1, \dotsc , r\}$, тогда результирующая матрица соответствующая группе $I$ имеет вид:
$\mathbf{X}_{I} = \mathbf{X}_{1} + \dotsc + \mathbf{X}_{r}$.

\section{Этап 4. Диагональное усреднение}
Пусть $\mathbf{Y}$ --- матрица $L \times K, L < K$. $y_{ij}$ - элементы матрицы, где $1 \leqslant i \leqslant L, 1 \leqslant j \leqslant K$. Также пусть $N = L + K - 1$.  Диагональное усреднение преобразует матрицу $\mathbf{Y}$ в ряд $g_0, \dotsc , g_{N-1}$ по формуле:

\begin{equation*}
	g_k = 
	\begin{cases}
		\dfrac{1}{k+1} \sum \limits_{m=1}^{k+1} y_{m, k-m+2} &  \text{, для } 0 \leqslant k < L-1\\
		\dfrac{1}{L} \sum \limits_{m=1}^L y_{m, k-m+2} &  \text{, для } L-1 \leqslant k < K\\
		\dfrac{1}{N-k} \sum \limits_{m=k-K+2}^{N-K+1} y_{m, k-m+2} & \text{, для } K \leqslant k < N 
	\end{cases}
\end{equation*}

Применяя диагональное усреднение к результирующей матрице группы $I$, получаем ряд $\hat{\mathsf{F}} = (f_1 \cdots f_{N-1})$. Полученный ряд $\hat{\mathsf{F}}$ назовем оценкой сигнала, полученной с помощью SSA.
Процедуру выделения сигнала с помощью SSA обозначим как: 
$$ \hat{\mathsf{F}} = SSA_{L, r}(\mathsf{F}), $$ 
где L --- длина окна в SSA, r --- количество первых собственных троек, участвующие в построении $\hat{\mathsf{F}}$.




\chapter{Использование SSA в машинном обучении}
\section{Задача}
Рассмотрим $\mathsf{Z}_N$ --- временной ряд длины $N$ и задачу: с помощью модели некоторой нейронной сети на основе $T$ последовательных точек ряда $\mathsf{Z}_N$, предсказать следующие $R$ точек ряда. 
Решение данной задачи, можно разбить на несколько частей: подготовка данных, обучение модели, прогнозирование.

\section{Подготовка данных}

$\mathsf{Z}_N$ --- изначальный временной ряд длиной $N$.
Мы можем представить ряд в виде траекторной матрицы для длины окна $T + R$, тогда получим:

\begin{equation*} \mathbf{Z} = \mathcal{T}_{T+R}(\mathsf{Z}_N) =
	\begin{pmatrix} 
		z_1 & z_2 & \cdots & z_{T} & \vrule & z_{T+1} &  \cdots & z_{T+R-1} & z_{T+R} \\
		z_2 & z_3 & \cdots & z_{T+1} & \vrule & z_{T+2} &  \cdots & z_{T+R} & z_{T+R+1} \\
		\vdots & \vdots & \ddots & \vdots & \vrule & \vdots & \ddots & \vdots & \vdots  \\
		z_{N-T-R+1} & z_{N-T-R+2} & \cdots & z_{N-R} & \vrule & z_{N-R+1} &  \cdots & z_{N-1} & z_{N} \\
	\end{pmatrix}.
\end{equation*}

Матрица $\mathbf{Z}$ будет размерности $(N - T - R + 1) \times (T + R)$. Левую часть матрицы $\mathbf{Z}$ назовем $\mathbf{Z}^x$, а правую часть $\mathbf{Z}^y$. Также разобъем матрицу по строчкам на 3 части: train, val, test. Пусть $\tau$, $v$ и $t$ номера последних строчек в каждой соответствующей части. \\
Обозначим с помощью $\mathbf{Z}_{a,b}^{(c, d)}$ часть матрицы $\mathbf{Z}$ с $a$ по $b$ строчку и с $c$ по $d$ столбец. Тогда train, val, test части записываются как: $\mathbf{Z}_{train} = \mathbf{Z}_{1,\tau}^{(1, T+R)}, \mathbf{Z}_{val} = \mathbf{Z}_{\tau+T+R,v}^{(1, T+R)}, \mathbf{Z}_{test} = \mathbf{Z}_{v+T+R,t}^{(1, T+R)}$. В этих же обозначениях $\mathbf{Z}^x = \mathbf{Z}_{1,t}^{(1, T)}, \mathbf{Z}^y = \mathbf{Z}_{1,t}^{(T+1, T+R)}$.

\subsubsection{SSA-preprocessing}
\label{SSA-preprocessing}
Предобработка SSA для тренировочной выборки описывается следующим алгоритмом. $L, r$ --- гипер-параметры, описанные в разделе <<1.1. Алгоритм SSA>>:

\begin{enumerate}
	\item Преобразуем train часть матрицы $\mathbf{Z}$ во временной ряд $\widetilde{\mathsf{Z}} = \mathcal{T}^{-1}(\mathbf{Z}_{train})$.
	\item Получим ряд $\hat{\mathsf{Z}} = SSA_{L, r}(\widetilde{\mathsf{Z}})$.
	\item Получаем траекторную матрицу $\mathbf{\hat{Z}} = \mathcal{T}_{T + R}(\hat{\mathsf{Z}})$.
	\item Полученная траекторная матрица $\mathbf{\hat{Z}}$ будет результатом работы предобработки SSA для тренировочной выборки.
\end{enumerate}

Предобработка SSA для валидационной или тестовой выборки отличается от предыдущей, ввиду разных предназначений выборок. В отличии от тренировочной выборки о которой мы знаем все, считается, что о валидационной и тестовой выборках ничего не известно. В этих случаях SSA-обработку следует применять так, чтобы предыдущие значения ряда не получали информации от будущих (<<заглядывание в будущее>>).

Пусть $\mathsf{Z}_{b, e} = [z_b, z_{b+1}, \cdots, z_e]$ подряд ряда $ \mathsf{Z} $, где $ b $ --- начальный индекс, $ e $ --- конечный индекс. Пусть $ p $ --- тоже индекс ряда, такой что $b < p < e$. Следующий алгоритм описывает процедуру получения ряда $\mathsf{Z}_{p+1, e}$, обработанного с помощью SSA без <<заглядывание в будущее>>:

\begin{enumerate}
	\item Пусть есть ряд $\mathsf{Z}_{b, e}$ и задано $p$. Тогда $Q = e - p$ --- размер ряда $ \mathsf{Z}_{p+1, e} $. Пусть $ \hat{\mathsf{Z}}_Q = (\hat{z}_1, \cdots, \hat{z}_Q) $ - ряд размера $Q$.
	\item Для каждого $ i = [1, \cdots, Q] $ получим $\hat{\mathsf{Z}}^{'}_{b+i-1, p+i} = SSA_{L, r}(\mathsf{Z}_{b+i-1, p+i})$, присвоим значение последнего элемента полученного ряда $\hat{z}^{'}_{p+i}$ значению ряда $\hat{\mathsf{Z}}_Q$ с соответствующим индексом, то есть $\hat{z}_i = \hat{z}^{'}_{p+i}$.
	\item Получили ряд $\hat{\mathsf{Z}}_Q$ размера $ Q $, значения которого являются значениям ряда  $\mathsf{Z}_{p+1, e}$, обработанные с помощью SSA без <<заглядывания в будущее>>.
\end{enumerate}

Процедуру получения $\hat{\mathsf{Z}}_Q$ обозначим: $\hat{\mathsf{Z}}_Q = \mathcal{SSA}^{(p)}(\mathsf{Z}_{b, e})$. Тогда алгоритм предобработки для валидационной выборки запишется следующим образом:
\begin{enumerate}
	\item Запишем $\mathbf{Z}_{1,v}^{(1, T+R)}$ как $\mathsf{Z}_{1, v + T + R}$.
	\item Выберем $p = \tau + T + R$.
	\item Получим $\hat{\mathsf{Z}}_Q = \mathcal{SSA}^{(p)}(\mathsf{Z}_{1, v + T + R})$.
	\item Перейдем обратно к траекторной матрице $\hat{\mathbf{Z}}_{val} = \mathcal{T}_{T+R}(\hat{\mathsf{Z}}_Q) $.
\end{enumerate}
$\hat{\mathbf{Z}}_{val}$ --- будет результатом предобработки SSA для валидационной выборки. Размерность $\hat{\mathbf{Z}}_{val}$ будет совпадать с размерностью $\mathbf{Z}_{val}$.

Запишем аналогичный алгоритм для тестовой выборки:

\begin{enumerate}
	\item Запишем $\mathbf{Z}_{1,t}^{(1, T+R)}$ как $\mathsf{Z}_{1, t + T + R} = \mathsf{Z}_N$.
	\item Выберем $p = v + T + R$.
	\item Получим $\hat{\mathsf{Z}}_Q = \mathcal{SSA}^{(p)}(\mathsf{Z}_N)$.
	\item Перейдем обратно к траекторной матрице $\hat{\mathbf{Z}}_{test} = \mathcal{T}_{T+R}(\hat{\mathsf{Z}}_Q) $.
\end{enumerate}
$\hat{\mathbf{Z}}_{test}$ --- будет результатом предобработки SSA для тестовой выборки.

\section{Модель}

\paragraph{Обучение} Оптимизация параметров модели проводится с помощью процедуры обратного распространения ошибки на тренировочной выборке. Модель учится по $\mathbf{Z}^x_{train}$ предсказывать $\mathbf{Z}^y_{train}$. Эпоха --- цикл прохода всех строчек из тренировочной выборки в обучении. Количество эпох для обучения является гипер-параметром. На валидационной выборки оценивается оптимальное количество эпох, нужных для модели (дабы избежать переобучения).

Перед началом обучения нужно выбрать гипер-параметры модели и количество эпох.

Алгоритм обучения модели после выбора архитектуры:

\begin{enumerate}
	\item Инициализация модели со случайными весами. 
	\item На тренировочной выборке $\mathbf{Z}_{train}$ оптимизируются веса $w, \theta$ с заданным количеством эпох. Модель учится по данным строчкам $\mathbf{Z}^x_{train}$ предсказывать соответствующие строчки $\mathbf{Z}^y_{train}$. Для каждой $i$-ой эпохи считается $\epsilon_i$ -- ошибка на валидационной выборке. Для валидационной выборки $\mathbf{Z}^x_{val}$ строится прогноз $\hat{\mathbf{Z}}^y_{val}$. Ошибка $\epsilon_i$ получается сравнением $\hat{\mathbf{Z}}^y_{val}$ с $\mathbf{Z}^y_{val}$ по какой-нибудь метрике (например MSE). 
	\item Находим $i: \min(\epsilon_i)$.
	\item Веса модели сбрасываются (снова инициализация случайными весами).
	\item Снова на тренировочной выборке оптимизируются веса $w, \theta$ с количеством эпох равных $i$.
\end{enumerate}

\section{Прогнозирование}
После того как модель обучена, можно перейти к прогнозированию точек ряда.

\begin{enumerate}
	\item Возьмем  $\mathbf{Z}^x_{test}$ и $\mathbf{Z}^y_{test}$.
	\item Представим $\mathbf{Z}^x_{test} = [Z_{test}^{x, 1} : \cdots : Z_{test}^{x, Q}]^T, $ где $Q$ --- количество строчек в тестовой матрицы $ \mathbf{Z}_{test} $.
	\item Для каждой строчки матрицы $\mathbf{Z}^x_{test}$ получаем прогноз с помощью обученной модели. Запишем результат прогноза как матрицу $ \mathbf{\hat{Z}}^y = [\hat{Z}^{y, 1} : \cdots : \hat{Z}^{y, Q}]^T$.

	\item Далее можно сравнить $\mathbf{\hat{Z}}^y $ с $\mathbf{Z}^y_{test}$ по какой-нибудь метрике.
\end{enumerate}

\section{Метрики}
С помощью метрик MSE и RMSE можно мерить размер ошибки полученного прогноза.
$$ MSE(\mathbf{Z}^y_{test}, \mathbf{\hat{Z}}^y) = \frac{1}{Q} diag((\mathbf{Z}^y_{test} - \mathbf{\hat{Z}}^y) (\mathbf{Z}^y_{test} - \mathbf{\hat{Z}}^y)^T) $$ 
$$ RMSE(\mathbf{Z}^y_{test}, \mathbf{\hat{Z}}^y) = \sqrt{MSE(\mathbf{Z}^y_{test}, \mathbf{\hat{Z}}^y)} $$ 

\chapter{Модели}
\label{models}
В данной главе рассматриваются нейронные сети используемые в работе. Всего рассмотрено четыре архитектуры. Одна линейная, в дальнейшем называется ANN, и три рекуррентных нейронных сети: RNN, LSTM, GRU.

\section{Artificial Neural Network (ANN)}

Artificial Neural Network (ANN) включает в себя входной слой, ряд скрытых слоев и выходной слой, каждый слой содержит несколько узлов. Считается, что ANN с одним скрытым слоем обеспечивает достаточную сложность для моделирования нелинейных взаимосвязей данных. ANN в этой работе формализуется следующим образом: 

$$ \hat{X}^T = [\hat{x}_1, \cdots, \hat{x}_T] $$ -- входные данные, на которых модели учиться делать предсказания.
$$ \hat{Y}^T = [\hat{y}_1, \cdots, \hat{y}_R] $$ -- выходные данные, предсказания модели.

$$ y_k = \phi_2\bigg(\sum\limits_{j=1}^{h} w^{(2)}_{jk} \phi_1(\sum\limits_{i=1}^{T} w^{(1)}_{ij} x_{i} + \theta^{(1)}_{j})  + \theta^{(2)}_k \bigg), k = [1, \cdots, R], $$
где $T$ --- размер входного вектора на котором выполняется прогноз, $h$ --- размер скрытого слоя. $w$ и $\theta$ --- веса модели. $\phi$ --- функция активации. $R$ --- размер выходного вектора-прогноза.

Ниже представлен список некоторых функций активаций:

\begin{enumerate}
	\item Линейная функция активации: $\phi(x) = x$.
	\item Сигмоида: $\phi(x) = \dfrac{1}{1 + e^{-x}}$.
	\item ReLU: $\phi(x) = \begin{cases} 
		0, & x < 0 \\
		x, & x \geqslant 0
	\end{cases}$
\end{enumerate}

\section{Recurrent neural network (RNN)}

Модель recurrent neural network (RNN), как следует из названия, используют рекурсию в своих архитектурах для решения задач, где данные содержатся в некоторой последовательности (например, временные ряды, текстовые задачи и др.). В RNN присутствует вектор скрытого слоя, служащий для <<накопления>> информации.

$\hat{X}^T = [\hat{x}_1, \cdots, \hat{x}_T]$ -- вектор входных данных. 

$ H^T_t = [h_1, \cdots, h_J]$ -- вектор скрытого слоя в момент $t$. 

$ \hat{Y}^T = [\hat{y}_1, \cdots, \hat{y}_R] $ -- вектор выходных данных в момент $T$. 

Следующие формулы описывают модель:

\begin{gather*} 
	H_t = f_h(U x_{t} + \theta_1 + W H_{t-1} + \theta_2), \\
   	Y_t = f_y(V H_t + \theta_3),
\end{gather*}

где $t = [1, \cdots, T]$,  $U, V, W, \theta_i$ -- веса модели, вычисляющиеся в процессе обучения, $f_h, f_y$ -- функции активации (в большинстве случаев сигмоиды).

\section{Long short-term memory(LSTM)}

Long short-term memory (LSTM) --- разновидность рекуррентных моделей, с добавлением второго скрытого слоя, используемого для <<долгосрочной>> памяти.

$\hat{X}^T = [\hat{x}_1, \cdots, \hat{x}_T]$ -- вектор входных данных. 

$ H^T_t = [h_1, \cdots, h_J]$ -- вектор скрытого слоя в момент $t$. 

$ C^T_t = [c_1, \cdots, c_J]$ -- вектор скрытого слоя в момент $t$. 

$\hat{Y}^T = [\hat{y}_1, \cdots, \hat{y}_R] $ -- вектор выходных данных в момент $T$. 

Следующие формулы описывают модель:

\begin{gather*} 
	f_t = \sigma(W_f \cdot [H_{t-1}, x_t] + b_f), \,
	i_t = \sigma(W_i \cdot [H_{t-1}, x_t] + b_i), \\
	\tilde{C}_t = \tanh(W_c \cdot [H_{t-1}, x_t] + b_c), \,
	C_t = f_t * C_{t_1} + i_t * \tilde{C}_t, \\
	o_t = \sigma(W_o \cdot [H_{t-1}, x_t] + b_o), \,
	H_t = o_t * \tanh(C_t), \\
	Y_t = f(V H_t + b_0),
\end{gather*}

где $t = [1, \cdots, T]$,  $W, b$ -- веса модели, вычисляющиеся в процессе обучения, $f$ -- функция активации. Оператор $*$ -- производит поэлементное умножение.

\section{Gated Recurrent Unit (GRU)}

Gated Recurrent Unit (GRU) --- модель похожая на LSTM, но без дополнительного скрытого слоя.

$\hat{X}^T = [\hat{x}_1, \cdots, \hat{x}_T]$ -- вектор входных данных. 

$ H^T_t = [h_1, \cdots, h_J]$ -- вектор скрытого слоя в момент $t$. 

$\hat{Y}^T = [\hat{y}_1, \cdots, \hat{y}_R] $ -- вектор выходных данных в момент $T$. 

Следующие формулы описывают модель:

\begin{gather*} 
	z_t = \sigma(W_z \cdot [H_{t-1}, x_t] + b_z), \,
	r_t = \sigma(W_r \cdot [H_{t-1}, x_t] + b_r), \\
	\tilde{H}_t = \tanh(W \cdot [r_t * H_{t-1}, x_t] + b), \,
	H_t = (1 - z_t) * H_{t-1} + z_t * \tilde{H}_t, \\
	Y_t = f(V H_t + b_0),
\end{gather*}

где $t = [1, \cdots, T]$,  $W, b$ -- веса модели, вычисляющиеся в процессе обучения, $f$ -- функция активации. Оператор $*$ -- производит поэлементное умножение.

\chapter{Синтетические данные}
\section{Синус с шумом}
\label{noisy_sine}
Рассмотрим следующий ряд $\mathsf{Z}_{1500}$, состоящий из элементов $z_i = \sin(2 \pi \omega i ) + \varepsilon_i$, где $\omega$ -- частота, равная $\dfrac{1}{12}$, $\varepsilon_i$ -- шум из стандартного нормального распределения $N(0, 1)$. 

\begin{figure}[h]
	\center{\includegraphics[width=0.7\linewidth]{1}}
	\caption{Часть ряда $\mathsf{Z}_{1500}$.}
	\label{pic_nsine}
\end{figure}

\subsection{Сравнение модели ANN и гибридной модели SSA-ANN}
\label{raw_hybrid_models_comp_nsine}
Поставим задачу предсказывать следующую точку синуса по двенадцати предыдущим, то есть возьмем $T = 12$ и $R = 1$. В рамках этой задачи хотим сравнить обычную модель ANN и гибридные модели SSA-ANN по метрике RMSE. Также мы будем сравнивать отклонение полученного прогноза от сигнала $\sin(2 \pi \omega i )$ и самого ряда $\mathsf{Z}_{1500}$.

Мы будем рассматривать две версии гибридных моделей SSA-ANN. В первой версии обработка методом SSA будет применяться для левой части $\mathbf{Z}^x$ траекторной матрицы. Таким образом мы хотим проверить гипотезу о том, что точность ANN повысится, если прогноз будет строиться на данных, очищенных от шума. Такие модели будем называть SSA-X-ANN. Во второй версии только правая часть $\mathbf{Z}^y$ обрабатывается с помощью метода SSA. В этом случае мы хотим проверить гипотезу, что точность модели ANN повысится, если модель будет обучаться прогнозировать данные без шума. Такие модели будем называть SSA-Y-ANN.

\subsubsection{Модели}
Для предсказаний воспользуемся нейронной сетью ANN, описанной в разделе <<\ref{models} Модели>>. Назовем ANN-1 -- модель без скрытых слоев и обе функций активации являются линейными. Она соответствует обычной линейной регрессии. ANN-2 -- назовем модель с одним скрытым слоем, размера $50$. $\phi_1$ -- является функцией активации ReLU, а $\phi_2$ -- линейная функция активации. SSA-ANN-1 и SSA-ANN-2 назовем группы гибридных версии для каждой модели. Стоит отметить, что гибридные модели могут быть разными, мы будем рассматривать по две гибридные модели, которые были описаны в разделе ранее, для каждой обычной (например, SSA-X-ANN-1 или SSA-Y-ANN-2). 
\subsubsection{Подготовка данных}
Построим траекторную матрицу для ряда $\mathsf{Z}_{1500}$ с окном длиной $13$. 

\begin{equation*} \mathbf{Z} = \mathcal{T}_{13}(\mathsf{Z}_{1500}) =
	\begin{pmatrix} 
		z_1 & z_2 & \cdots & z_{12} & \vrule & z_{13} \\
		z_2 & z_3 & \cdots & z_{13} & \vrule & z_{14} \\
		\vdots & \vdots & \ddots & \vdots & \vrule & \vdots \\
		z_{1488} & z_{1489} & \cdots & z_{1499} & \vrule & z_{1500}  \\
	\end{pmatrix}.
\end{equation*}

Разобъем данные на тренировочную, валидационную и тестовую выборку следующим образом: $\tau = 988$, $v = 1238$ и $t = 1488$.
Далее для моделей ANN-1 и ANN-2 будем использовать уже полученную матрицу $\mathbf{Z}$ без каких-то обработок методом SSA. Параметры оператора $SSA_{L, r}$ задаются следующим образом $L = \dfrac{\tau + 12}{2}$, $r = 2$. Для гибридных моделей SSA-X-ANN-1 и SSA-X-ANN-2 обработаем только левую часть матрицы $\mathbf{Z}$ методом SSA, как описано в разделе <<\ref{SSA-preprocessing} SSA-preprocessing>>. Для гибридных моделей SSA-Y-ANN-1 и SSA-Y-ANN-2 обработаем только правую часть матрицы $\mathbf{Z}$ методом SSA. Таким образом получили для каждой модели свою матрицу $\mathbf{Z}$.

\subsubsection{Результаты}
В ходе эксперимента полученные результаты для моделей ANN-1 и SSA-ANN-1 представлены в таблице \ref{res_ANN-1}. Результаты для моделей ANN-2 и SSA-ANN-2 представлены в таблице \ref{res_ANN-2}.

\begin{table}[h]
	\centering
	\caption{RMSE для моделей ANN-1 и SSA-ANN-1.}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& ANN-1 & SSA-X-ANN-1 & SSA-Y-ANN-1 \\ \hline
		Signal      & 0.414 & 0.086                  & 0.409                  \\ \hline
		Time series & 1.043 & 0.998                  & 1.037                  \\ \hline
	\end{tabular}
	\label{res_ANN-1}
\end{table}

\begin{table}[h]
	\centering
	\caption{RMSE для моделей ANN-2 и SSA-ANN-2.}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& ANN-2 & SSA-X-ANN-2 & SSA-Y-ANN-2 \\ \hline
		Signal      & 0.415 & 0.014                  & 0.365                  \\ \hline
		Time series & 1.045 & 1.007                  & 1.032                  \\ \hline
	\end{tabular}
	\label{res_ANN-2}
\end{table}


На результатах выше можно наблюдать, что гибридные модели показывают результаты лучше, чем обычная модель. Модели SSA-Y-ANN показывают результат сравнимый с обычной моделью, можно сказать, что в этом случае улучшение незначительное. В случае моделей SSA-X-ANN видно хорошее улучшение. Наблюдается снижение ошибки по RMSE на 0.4, если считать отклонение от сигнала. Это говорит о том, что гибридная модель SSA-X-ANN хорошо предсказывает сигнал ряда. Также посмотрим на ошибки для сигнала и ряда в случае SSA-X-ANN. Видно, что квадраты ошибок отличаются, примерно, на дисперсию шума $\sigma^2$, что говорит о том, что метод SSA в модели SSA-X-ANN удалил из ряда шумовые компоненты. В случае же моделей ANN таких результатов не наблюдается. Из этого следует, что модели SSA-X-ANN в отличии от ANN пытается прогнозировать чистый сигнал, что дает лучшие результаты. 

\subsubsection{Выводы}

Можно заключить, что для обоих архитектур моделей наблюдается одна тенденция: гибридные модели показывают результаты лучше, чем обычные. В частности, гибридные модели SSA-X-ANN показывает наилучшие результаты. Можно сказать, что правильное использование гибридных моделей SSA-ANN дает прирост в точности в задаче прогнозирование синуса с шумом.

\subsection{Подбор гипер-параметров моделей}
\label{hyper-params-sine}
Поставим задачу подобрать гипер-параметры к моделям ANN, RNN, LSTM и GRU. В этот раз данные не будут предобработаны с помощью SSA. $T = 12, 24$ и $R = 1$. Модели будем сравнивать с помощью метрики RMSE. Главной целью этого исследования стоит задача выяснить наилучшую конфигурацию модели для каждой архитектуры. Будем считать ошибку от ряда $\mathsf{Z}_{1500}$, а также от его сигнала $\sin(2 \pi \omega i )$.

\subsubsection{Модели}

Воспользуемся моделями, описанными в главе \ref{models}. Так как синус можно предсказывать по линейно рекуррентной формуле, то будет рассматривать функции активации линейную и ReLU. Рассмотрим модели поподробнее.

\paragraph{ANN} Возьмем $\phi_1$ -- ReLU, а $\phi_2$ -- линейной функцией активации. Тогда в качестве настраиваемого гипер-параметра останется размер скрытого слоя. Его и будем подбирать.

\paragraph{RNN} Возьмем $f_h$ -- ReLU, а $f_y$ -- линейной функцией активации. Будем подбирать размер скрытого слоя $H$.

\paragraph{LSTM} $f$ -- линейная функция активации. Также подбираем размеры скрытых слоев $H$ и $C$ (в архитектуре LSTM они имеют одинаковый размер).

\paragraph{GRU} $f$ -- линейная функция активации. Подбираем размер скрытого слоя $H$.

В итоге получаем, что для каждой модели подбираем размер скрытого слоя. Будем перебирать по сетке от $10$ до $100$ с шагом $5$.

\subsubsection{Подготовка данных}

Построим траекторные матрицы для ряда $\mathsf{Z}_{1500}$ с окном длиной $13$ и $25$. 

\begin{gather*} 
	\mathbf{Z}^{12} = \mathcal{T}^{12}_{13}(\mathsf{Z}_{1500}) \\
	\mathbf{Z}^{24} = \mathcal{T}_{25}(\mathsf{Z}_{1500})
\end{gather*}

Разобъем данные на тренировочную, валидационную выборку следующим образом: $\tau = 988$, $v = 1488$ для $T = 12$, и $\tau = 976$, $v = 1476$ для $T = 24$. Далее для моделей, описанных выше, будем использовать полученные матрицы $\mathbf{Z}^{12}$ и $\mathbf{Z}^{24}$. Так как мы подбираем гипер-параметры, тестовая выборка не нужна.

\subsubsection{Результаты}

Рассмотрим результаты для $T = 12$ и $T = 24$ вместе. Результаты представлены в виде графика, где на оси $x$ отложена сетка по которой перебирался размер скрытого слоя, по оси $y$ -- ошибка RMSE. По точкам на графике нарисована линия регрессии, по ней можно понять общую тенденцию на графике, также в описания каждого графика указано p-value для линии регрессии. Так как хотелось бы лучше увидеть разницу между результатами, полученными при $T = 12$ и $T = 24$, для каждой модели они отображаются на одном графике. Синим цветом -- $T = 12$, оранжевым -- $T = 24$.

Для каждой линии регрессии будем проверять ее значимость с уровнем значимости $0.1$. Если линия не значима, будем выбирать точки ближе к середине оси $x$. Если же линия значима, выбираем те точки с краю, на котором линия регрессии убывает. Также не будем брать точки, которые находятся далеко от линии регрессии. Выбирать размер скрытого слоя будем по линиям регрессии построенным на временном ряде.

\paragraph{ANN} Посмотрим на рисунки \ref{res_sine_ann_series}, \ref{res_sine_ann_signal}. Видно, что ошибки при $T=24$ ниже, чем при $T=12$ в обоих случаях. Смотрим на p-value, линии регрессии незначимые, кроме оранжевой на рис. \ref{res_sine_ann_series} при уровне значимости $0.1$. В этом случае выбираем размер скрытого слоя поближе к $100$, в остальных будем выбирать размер ближе к $50$. 

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/series/ann}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Time Series ANN: p-value для 12 точек: 0.6457, p-value для 24 точек: 0.0665.}
	\label{res_sine_ann_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/signal/ann}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Signal ANN: p-value для 12 точек: 0.4787, p-value для 24 точек: 0.1632.}
	\label{res_sine_ann_signal}
\end{figure}


\paragraph{RNN} Посмотрим на рисунки \ref{res_sine_rnn_series}, \ref{res_sine_rnn_signal}. Наблюдаем такую же тенденцию как и в случае с ANN, результаты для $T=24$ лучше, чем для $T=12$. На рисунке \ref{res_sine_rnn_series} линия регрессии для 12 точек незначимая, следовательно размер скрытого слоя выбираем ближе к середине, а для 24 -- значимая, выбираем размер скрытого слоя ближе к $100$. В случае сигнала все меняется. На рисунке \ref{res_sine_rnn_signal} видно, что в случае 12 точек линия -- значимая, в случае 24 -- незначимая.
\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/series/rnn}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Time Series RNN: p-value для 12 точек: 0.2891, p-value для 24 точек: 0.0493.}
	\label{res_sine_rnn_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/signal/rnn}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Signal RNN: p-value для 12 точек: 0.0029, p-value для 24 точек: 0.1331.}
	\label{res_sine_rnn_signal}
\end{figure}

\paragraph{LSTM} Посмотрим на рисунки \ref{res_sine_lstm_series}, \ref{res_sine_lstm_signal}. Такая же тенденция, как и в случае с ANN и RNN. Смотрим на p-value, видно что только для случае сравнения с временным ряда для $ T=24 $ линия регрессии незначимая, в этом случае стоит брать размер скрытого слоя ближе к $50$. Для $T=12$ -- ближе к $100$.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/series/lstm}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Time Series LSTM: p-value для 12 точек: 0.0006, p-value для 24 точек: 0.3559.}
	\label{res_sine_lstm_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/signal/lstm}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Signal LSTM: p-value для 12 точек: 0.0043, p-value для 24 точек: 0.0115.}
	\label{res_sine_lstm_signal}
\end{figure}

\paragraph{GRU} Посмотрим на рисунки \ref{res_sine_gru_series}, \ref{res_sine_gru_signal}. Сохраняется тенденция как и в случаях с ANN, RNN, LSTM. Значимость такая же как и в случае с LSTM, размер скрытого слоя выбирается аналогично.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/series/gru}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Time Series GRU: p-value для 12 точек: 0.0102, p-value для 24 точек: 0.2568.}
	\label{res_sine_gru_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/sine/compare/signal/gru}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Signal GRU: p-value для 12 точек: 0.0003, p-value для 24 точек: 0.0045.}
	\label{res_sine_gru_signal}
\end{figure}

\subsubsection{Выводы}

После полученных результатов для каждой модели можно зафиксировать размер скрытого слоя и размер $T$. $T = 24$, ANN - $100$, RNN - $65$, LSTM - $55$, GRU - $100$. В дальнейших исследованиях для этого набора данных будем использовать эти модели с такими гипер-параметрами.

\subsection{Сравнение гипер-параметров SSA в гибридных моделях}
\label{hyper-params-ssa-sine}
Поставим задачу сравнить параметр SSA $r$ в моделях ANN, RNN, LSTM и GRU. $T = 24$ и $R = 1$. Модели и гибридные модели будем сравнивать с помощью метрики RMSE. Будем считать ошибку от ряда $\mathsf{Z}_{1500}$, а также от его сигнала $\sin(2 \pi \omega i )$. 

По теории для ряда $\mathsf{Z}_{1500}$ $r = 2$. Также проверим случаи $r = 1$ и $r = 4$.

\subsubsection{Модели}

Будем использовать модели полученные в разделе \ref{hyper-params-sine}.

\subsubsection{Подготовка данных}

Построим траекторную матрицу для ряда $\mathsf{Z}_{1500}$ с окном длиной $25$. 

\begin{equation*} \mathbf{Z} = \mathcal{T}_{25}(\mathsf{Z}_{1500})
\end{equation*}

Разобъем данные на тренировочную, валидационную и тестовую выборку следующим образом: $\tau = 976$, $v = 1226$ и $t = 1476$.
Далее для обычных моделей будем использовать уже полученную матрицу $\mathbf{Z}$ без каких-то обработок методом SSA. Положим $L = \dfrac{\tau + 24}{2}$. Для гибридных моделей обработаем только левую часть матрицы $\mathbf{Z}$ методом SSA, как описано в разделе <<\ref{SSA-preprocessing} SSA-preprocessing>>. Таким образом получили для каждой модели свою матрицу $\mathbf{Z}$.

\subsubsection{Результаты}

Посмотрим на результаты в таблицах \ref{table_sine_ssa_series}, \ref{table_sine_ssa_signal}. Видно, что среди обычных моделей лидирует GRU. Для случая $r=1$ результаты гибридных моделей хуже, чем у обычных моделей. Это объяснимо, так как по теории здесь нужно взять две компоненты, в случае $r=1$ мы не добрали еще одну компоненту. В случае $r = 2$ гибридные модели показывают наилучшие результаты. В частности SSA-RNN имеет наименьшую ошибку $0.99646$ (в сравнении с сигналом данная модель также показывает лучшие результаты). Для $r = 4$ ситуация выглядит немного хуже, так как в сигнал, выделенный SSA попал небольшой шум.

\begin{table}[H]
	\centering
	\caption{Time Series: RMSE error.}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& ANN     & RNN     & LSTM    & GRU     \\ \hline
		raw    & 1.04703 & 1.04161 & 1.03374 & 1.02496 \\ \hline
		r = 1 & 1.27891 & 1.14262 & 1.08114 & 1.10521 \\ \hline
		r = 2 & 1.01494 & 0.99646 & 1.01255 & 1.01673 \\ \hline
		r = 4 & 1.01679 & 1.02385 & 1.03374 & 1.01549 \\ \hline
	\end{tabular}
	\label{table_sine_ssa_series}
\end{table}

\begin{table}[H]
	\centering
	\caption{Signal: RMSE error.}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& ANN     & RNN     & LSTM    & GRU     \\ \hline
		raw   & 0.35654 & 0.29322 & 0.27239 & 0.28205 \\ \hline
		r = 1 & 0.73874 & 0.49316 & 0.35381 & 0.41770 \\ \hline
		r = 2 & 0.15457 & 0.11132 & 0.15909 & 0.16713 \\ \hline
		r = 4 & 0.1514  & 0.22734 & 0.16797 & 0.15862 \\ \hline
	\end{tabular}
	\label{table_sine_ssa_signal}
\end{table}



\subsubsection{Выводы}

Можно заключить, что правильное или избыточное количество компонент в предобработки SSA уменьшают ошибку для данного ряда. Наименьшую ошибку $1.02496$ среди обычных моделей показала GRU. Среди гибридных моделей -- SSA-RNN с ошибкой $0.99646$.

\section{Два синуса с близкими периодами}
\label{noisy_dsine}
Рассмотрим следующий ряд $\mathsf{Z}_{1500}$, состоящий из элементов $$ z_i = (\sin(2 \pi \dfrac{i}{T} ) + 1.6 \sin(2 \pi \dfrac{i}{T + 2} ) ) \cdot (\cos(2 \pi \dfrac{i}{432}) + 5) + 9.52 \varepsilon_i, $$ где $T = 12$, а $\varepsilon_i$ -- шум из стандартного нормального распределения $N(0, 1)$. 

\begin{figure}[h]
	\center{\includegraphics[width=0.7\linewidth]{noisy_dsine}}
	\caption{Часть ряда $\mathsf{Z}_{1500}$.}
	\label{pic_ndsine}
\end{figure}


\subsection{Подбор гипер-параметров моделей}
\label{hyper-params-dsine}

Проведем такой же эксперимент с подбором гипер-параметров, как и в разделе \ref{hyper-params-sine}. В этот раз как средний период ряда равен $13$, рассмотрим $T = 13$ и $T = 26$. Цели эксперимента, модели и разбиение аналогичные, поэтому перейдем сразу к результатам.

\subsubsection{Результаты}

Рассмотрим результаты для $T = 13$ и $T = 26$ вместе. Графики построенны аналогично, как и в разделе \ref{hyper-params-sine}. Синим цветом -- $T = 13$, оранжевым -- $T = 26$. 

Для каждой линии регрессии будем проверять ее значимость с уровнем значимости $0.1$. Если линия не значима, будем выбирать точки ближе к середине оси $x$. Если же линия значима, выбираем те точки с краю, на котором линия регрессии убывает. Также не будем брать точки, которые находятся далеко от линии регрессии. Выбирать размер скрытого слоя будем по линиям регрессии построенным на временном ряде. Стоит сразу отметить, что тенденция преимущества большего $T$, как и в разделе \ref{hyper-params-sine}, здесь также сохраняется, кроме рис. \ref{res_dsine_ann_series} его рассмотрим отдельно. В остальных случаях будем сразу говорить о размере скрытого слоя.


\paragraph{ANN} Посмотрим на рисунки \ref{res_dsine_ann_series}, \ref{res_dsine_ann_signal}. На первом рисунке не видим общей тенденции, но можно сказать что линия для 26 точек незначимая для уровня значимости $0.1$. Для 13 точек линия регрессии значимая, выбираем размер скрытого слоя поближе к $100$, для 26 ближе к $50$.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/series/ann}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Time Series ANN: p-value для 13 точек: 0.0548, p-value для 26 точек: 0.1516.}
	\label{res_dsine_ann_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/signal/ann}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Signal ANN: p-value для 13 точек: 0.9071, p-value для 26 точек: 0.0032.}
	\label{res_dsine_ann_signal}
\end{figure}


\paragraph{RNN} Посмотрим на рисунки \ref{res_dsine_rnn_series}, \ref{res_dsine_rnn_signal}. Для 13 и 26 точек линии регрессии везде незначимые, выбираем размер скрытого слоя поближе к середине.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/series/rnn}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Time Series RNN: p-value для 13 точек: 0.6919, p-value для 26 точек: 0.3872.}
	\label{res_dsine_rnn_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/signal/rnn}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Signal RNN: p-value для 13 точек: 0.9115, p-value для 26 точек: 0.7056.}
	\label{res_dsine_rnn_signal}
\end{figure}

\paragraph{LSTM} Посмотрим на рисунки \ref{res_dsine_lstm_series}, \ref{res_dsine_lstm_signal}. Для 13 точек линия регрессии незначимая, выбираем размер скрытого слоя поближе к $50$, для 26 линии регрессии значима только на графике со временным рядом, выбираем размер ближе к $100$.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/series/lstm}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Time Series LSTM: p-value для 13 точек: 0.1728, p-value для 26 точек: 0.0527.}
	\label{res_dsine_lstm_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/signal/lstm}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Signal LSTM: p-value для 13 точек: 0.7746, p-value для 26 точек: 0.1665.}
	\label{res_dsine_lstm_signal}
\end{figure}

\paragraph{GRU} Посмотрим на рисунки \ref{res_dsine_gru_series}, \ref{res_dsine_gru_signal}. Для 13 и 26 точек все линии регрессии значимы, выбираем размер скрытого слоя поближе к $100$.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/series/gru}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Time Series GRU: p-value для 13 точек: 0.0036, p-value для 26 точек: 0.0059.}
	\label{res_dsine_gru_series}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/dsine/compare/signal/gru}}
	
	синий -- $T = 13$, оранжевый -- $T = 26$.
	\caption{Signal GRU: p-value для 13 точек: 0.0069, p-value для 26 точек: 0.0126.}
	\label{res_dsine_gru_signal}
\end{figure}


\subsubsection{Выводы}

После полученных результатов для каждой модели можно зафиксировать размер скрытого слоя и размер $T$. $T= 26$, ANN - $55$, RNN - $70$, LSTM - $80$, GRU - $100$. В дальнейших исследованиях для этого набора данных будем использовать эти модели с такими гипер-параметрами.

\subsection{Сравние гипер-параметров SSA в гибридных моделях}
\label{hyper-params-ssa-dsine}

Проведем такой же эксперимент со сравнением гипер-параметров, как и в разделе \ref{hyper-params-ssa-sine}. $L = 130, \dfrac{\tau + 26}{2}$, $T = 26$. Цели эксперимента такие же, используются модели из раздела \ref{hyper-params-dsine}, разбиение проводится аналогично, поэтому перейдем сразу к результатам.

\subsubsection{Результаты}

Сначала посмотрим на случай $L = 130$. Посмотрим на результаты в таблицах \ref{table_sine_dssa_130_series}, \ref{table_sine_dssa_130_signal}. Видно, что среди обычных моделей лидирует GRU. Для случаев $r=4, 8, 12. 16$ результаты гибридных моделей оказываются, лучше обычных моделей. Только для $r = 8$ и модели SSA-ANN, $r = 16$ и модели SSA-RNN результаты хуже, чем у обычных моделей. Не смотря на то, что правильное число компонент в этом ряде 12, наилучший результат показывает SSA-RNN при $r = 4$ в случае прогнозирования ряда. В случае же с сигналом лучшей моделью является SSA-LSTM при $r = 4$.

\begin{table}[H]
	\centering
	\caption{Time Series: L = 130, RMSE error.}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& ANN     & RNN     & LSTM   & GRU     \\ \hline
		raw    & 10.0203 & 10.0113 & 9.9198 & 10.0682 \\ \hline
		r = 4  & 9.6720  & 9.6387  & 9.665  & 9.7527  \\ \hline
		r = 8  & 10.0507 & 9.7322  & 9.7252 & 9.7552  \\ \hline
		r = 12 & 9.7937  & 9.8797  & 9.8661 & 9.8187  \\ \hline
		r = 16 & 9.9824  & 10.1346 & 9.879  & 9.905   \\ \hline
	\end{tabular}
	\label{table_sine_dssa_130_series}
\end{table}

\begin{table}[H]
	\centering
	\caption{Time Series: L = 130, RMSE error.}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& ANN    & RNN    & LSTM   & GRU    \\ \hline
		raw    & 4.2093 & 4.096  & 4.0024 & 4.503  \\ \hline
		r = 4  & 2.206  & 2.3964 & 2.1339 & 2.5055 \\ \hline
		r = 8  & 3.8976 & 2.8502 & 2.4311 & 2.6825 \\ \hline
		r = 12 & 3.1592 & 3.2036 & 2.5901 & 2.8049 \\ \hline
		r = 16 & 3.1775 & 3.5174 & 2.5237 & 2.7617 \\ \hline
	\end{tabular}
	\label{table_sine_dssa_130_signal}
\end{table}

Теперь перейдем к случаю $L = 130$. Посмотрим на результаты в таблицах \ref{table_sine_dssa_half_series}, \ref{table_sine_dssa_half_signal}. В этот раз все результаты гибридных моделей лучше, чем обычных. Наилучший результат показывает SSA-ANN при $r = 12$ в случае прогнозирования ряда. В случае же с сигналом лучшей моделью является SSA-GRU при $r = 8$.



\begin{table}[H]
	\centering
	\caption{Time Series: L = $\dfrac{\tau + 26}{2}$, RMSE error.}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& ANN     & RNN     & LSTM   & GRU     \\ \hline
		raw    & 10.0203 & 10.0113 & 9.9198 & 10.0682 \\ \hline
		r = 4  & 9.5322  & 9.7205  & 9.5465 & 9.5742  \\ \hline
		r = 8  & 9.5442  & 9.7658  & 9.6238 & 9.5965  \\ \hline
		r = 12 & 9.4907  & 9.7483  & 9.5908 & 9.6144  \\ \hline
		r = 16 & 9.5198  & 9.6957  & 9.6147 & 9.6166  \\ \hline
	\end{tabular}
	\label{table_sine_dssa_half_series}
\end{table}

\begin{table}[H]
	\centering
	\caption{Time Series: L = $\dfrac{\tau + 26}{2}$, RMSE error.}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& ANN    & RNN    & LSTM   & GRU    \\ \hline
		raw    & 4.2093 & 4.096  & 4.0024 & 4.503  \\ \hline
		r = 4  & 2.0693 & 3.0939 & 2.0285 & 2.0219 \\ \hline
		r = 8  & 2.2793 & 3.1766 & 2.3245 & 1.9417 \\ \hline
		r = 12 & 2.3813 & 3.0823 & 2.1882 & 1.9889 \\ \hline
		r = 16 & 2.2503 & 2.3004 & 2.011  & 2.0127 \\ \hline
	\end{tabular}
	\label{table_sine_dssa_half_signal}
\end{table}


\subsubsection{Выводы}

Также как и в аналогичном эксперименте, представленном в разделе \ref{hyper-params-ssa-sine}, можно заключить, что правильное или избыточное количество компонент в предобработки SSA уменьшают ошибку для данного ряда.  Наименьшую ошибку $9.9198$ среди обычных моделей показала GRU. Среди гибридных моделей -- SSA-RNN с ошибкой $9.4907$. Можно зафиксировать для этого ряда $L = \dfrac{\tau + 26}{2}$ и $r = 12$.



%==================================================================
%==================================================================
%==================================================================


\chapter{Реальные данные}
\section{Среднемесячные осадки в Индии}
\label{indian_rain}
Рассмотрим следующий ряд $\mathsf{Z}_{1500}$ (рис. \ref{indian_rain_img}) взятый из статьи \cite{ar1}. На ряде отображены среднемесячное количество осадков в Индии.

\begin{figure}[h]
	\center{\includegraphics[width=0.7\linewidth]{2}}
	\caption{Ряд $\mathsf{Z}_{1500}$.}
	\label{indian_rain_img}
\end{figure}

\subsection{Различное использовние SSA предобработки в гибридной модели SSA-ANN}
Поставим задачу предсказывать следующую точку ряда $\mathsf{Z}_{1500}$ по двенадцати предыдущим, то есть возьмем $T = 12$ и $R = 1$. В рамках этой задачи хотим сравнить обычную модель ANN и гибридные модели SSA-ANN по метрике RMSE. 
Также как и в главе <<\ref{noisy_sine} Синус с шумом>> рассмотрим две версии гибридных моделей SSA-ANN. В первой версии обработка методом SSA будет применяться для левой части $\mathbf{Z}^x$ траекторной матрицы. Во второй версии только правая часть $\mathbf{Z}^y$ обрабатывается с помощью метода SSA. Будем использовать такие же обозначения для этих моделей как и в разделе <<\ref{noisy_sine} Синус с шумом>>.

\subsubsection{Модели}

Для предсказаний воспользуемся теми же моделями, что и в задаче с синусом, описанными ранее в разделе \ref{raw_hybrid_models_comp_nsine}.

\subsubsection{Подготовка данных}

Построим траекторную матрицу для ряда $\mathsf{Z}_{1500}$ с окном длиной $13$. 

\begin{equation*} 
	\mathbf{Z} = \mathcal{T}_{13}(\mathsf{Z}_{1500}),
\end{equation*}
Разобъем данные на тренировочную, валидационную и тестовую выборку следующим образом: $\tau = 988$, $v = 1238$ и $t = 1488$.

Далее для модели ANN-2 будем использовать уже полученную матрицу $\mathbf{Z}$ без каких-то обработок методом SSA. Параметры оператора $SSA_{L, r}$ задаются следующим образом $L = \dfrac{\tau + 12}{2}$, $r = 7$. Для гибридных моделей SSA-X-ANN обработаем только левую часть матрицы $\mathbf{Z}$ методом SSA, как описано в разделе <<\ref{SSA-preprocessing} SSA-preprocessing>>. Для гибридных моделей SSA-Y-ANN обработаем только правую часть матрицы $\mathbf{Z}$ методом SSA. Таким образом получили для каждой модели свою матрицу $\mathbf{Z}$.

\subsubsection{Результаты}

В ходе эксперимента получились для моделей ANN-1 и SSA-ANN-1 следующие результаты, представленные на таблице \ref{res_rain_table_1}. А для моделей ANN-2 и SSA-ANN-2 на таблице \ref{res_rain_table_2}.

\begin{table}[h]
	\centering
	\caption{RMSE для моделей ANN-1 и SSA-ANN-1.}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& ANN-1 & SSA-X-ANN-1 & SSA-Y-ANN-1 \\ \hline
		RMSE  & 267.21 & 224.35                & 269.23               \\ \hline
	\end{tabular}
	\label{res_rain_table_1}
\end{table}

\begin{table}[h]
	\centering
	\caption{RMSE для моделей ANN-2 и SSA-ANN-2.}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& ANN-2 & SSA-X-ANN-2 & SSA-Y-ANN-2 \\ \hline
		RMSE  & 239.84 & 218.57                & 242.72               \\ \hline
	\end{tabular}
	\label{res_rain_table_2}
\end{table}

На таблице видна такая же тенденция, как и в задаче с синусом с шумом. Модели ANN и SSA-Y-ANN показывают сравнительно похожие результаты. А вот модели SSA-X-ANN показывают наилучшие результаты. Также видно значительное увеличение ошибки для моделей ANN-1 и ANN-2, это ожидаемый результат, так как сложность модели ниже ANN-1, чем у модели ANN-2. Но для моделей SSA-X-ANN-1 и SSA-X-ANN-2 разница в ошибке небольшая, что подчеркивает работу хорошую работу SSA в выделении сигнала.

Можно заключить, что с помощью гибридных моделей удалось улучшить точность прогнозирования реального ряда.

\subsubsection{Выводы}
Исходя из результатов полученных в этом эксперимент, и в разделе \ref{raw_hybrid_models_comp_nsine}, можно заключить, что правильное использование гибридных моделей SSA-ANN приводит к весьма хорошим улучшениям в задаче прогнозирования рядов.

\subsection{Количество точек для предсказания}
\label{rain_12_24_comp}
После выводов в экспериментах в разделах \ref{hyper-params-sine}, \ref{hyper-params-dsine} хотим проверить сохраниться ли тенденция, наблюдаемая в них. В этот раз данные не будут предобработанны с помощью SSA. $T = 12, 24$ и $R = 1$. Модели будем сравнивать с помощью метрики RMSE. Все модели и разбиение данных проводится аналогично как в разделе \ref{hyper-params-sine}.

\subsubsection{Результаты}

Рассмотрим результаты для $T = 12$ и $T = 24$ вместе. Графики построенны аналогично, как и в разделе \ref{hyper-params-sine}. Синим цветом -- $T = 12$, оранжевым -- $T = 24$. (Забегая вперед, скажем что тенденция будет сохранятся для всех моделей. Поэтому для экономии времени читателя будем рассматривать лучшие параметры только для случае $T = 24$)

Для каждой линии регрессии будем проверять ее значимость с уровнем значимости $0.1$. Если линия не значима, будем выбирать точки ближе к середине оси $x$. Если же линия значима, выбираем те точки с краю, на котором линия регрессии убывает. Также не будем брать точки, которые находятся далеко от линии регрессии. 

\paragraph{ANN} Посмотрим на рисунок \ref{res_rain_ann_series}. Видно, что тенденция наблюдаемая в разделе \ref{hyper-params-sine} сохраняется и на реальном ряде. В данном случае размер скрытого слоя стоит выбрать ближе к $50$, так как линия регрессии незначимая.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/indian_rain/series/ann}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$.
	\caption{Indian Rain ANN: p-value для 12 точек: 0.1356, p-value для 24 точек: 0.4538.}
	\label{res_rain_ann_series}
\end{figure}

\paragraph{RNN} Посмотрим на рисунок \ref{res_rain_rnn_series}. В данном случае размер скрытого слоя стоит выбрать ближе к $100$, так как линия регрессии значимая, в этот раз.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/indian_rain/series/rnn}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$
	\caption{Indian Rain RNN: p-value для 12 точек: 0.0005, p-value для 24 точек: 0.0072.}
	\label{res_rain_rnn_series}
\end{figure}

\paragraph{LSTM} Посмотрим на рисунок \ref{res_rain_lstm_series}. В данном случае размер скрытого слоя стоит выбрать ближе к $100$.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/indian_rain/series/lstm}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$
	\caption{Indian Rain LSTM: p-value для 12 точек: 0.2802, p-value для 24 точек: 0.001.}
	\label{res_rain_lstm_series}
\end{figure}

\paragraph{GRU} Посмотрим на рисунок \ref{res_rain_gru_series}. Также как и в случаях LSTM, RNN линия регрессии значима, но последние значения слишком сильно выходят из доверительного интервала, поэтому стоит взять размер скрытого слоя до $80$.

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/hyper-parameters/indian_rain/series/gru}}
	
	синий -- $T = 12$, оранжевый -- $T = 24$
	\caption{Indian Rain GRU: p-value для 12 точек: 0.5381, p-value для 24 точек: 0.0202.}
	\label{res_rain_gru_series}
\end{figure}

\subsubsection{Выводы}

После полученных результатов для каждой модели можно зафиксировать размер скрытого слоя и размер $T$. $T = 24$, ANN - $60$, RNN - $90$, LSTM - $95$, GRU - $65$. В дальнейших исследованиях для этого набора данных будем использовать эти модели с такими гипер-параметрами.


\subsection{Вклад SSA в улучшение прогноза реального ряда}
После выводов в экспериментах в разделах \ref{hyper-params-ssa-sine}, \ref{hyper-params-ssa-dsine} хотим проверить какой вклад внесет SSA на реальных данных. В этот раз возьмем $T = 24$ и $R = 1$. Возьмем модель GRU, которая показала себя наилучшей в экспериментах \ref{hyper-params-ssa-sine}, \ref{hyper-params-ssa-dsine}. Размер скрытого слоя будем перебирать по сетке от $10$ до $100$ с шагом $5$. Модели GRU и SSA-GRU будем сравнивать с помощью метрики RMSE. Все модели и разбиение данных проводится аналогично как в разделе \ref{hyper-params-sine}.

\subsubsection{Подготовка данных}

Построим траекторную матрицу для ряда $\mathsf{Z}_{1500}$ с окном длиной $13$. 

\begin{equation*} 
	\mathbf{Z} = \mathcal{T}_{13}(\mathsf{Z}_{1500}),
\end{equation*}
Разобъем данные на тренировочную, валидационную и тестовую выборку следующим образом: $\tau = 976$, $v = 1226$ и $t = 1476$.

Далее для обычной модели будем использовать уже полученную матрицу $\mathbf{Z}$ без каких-то обработок методом SSA. Параметры оператора $SSA_{L, r}$ задаются следующим образом $L = \dfrac{\tau + 24}{2}$, $r = 7$. Для гибридной модели обработаем только левую часть матрицы $\mathbf{Z}$ методом SSA, как описано в разделе <<\ref{SSA-preprocessing} SSA-preprocessing>>. Таким образом получили для каждой модели свою матрицу $\mathbf{Z}$.

\subsubsection{Результаты}

Посмотрим на картинку \ref{res_rain_gru_ssa_series}. На ней видно, что SSA имеет весомый вклад на этих данных. Также можно видеть, что с ростом скрытого слоя ошибка гибридной модели SSA-GRU падает. То есть SSA не ограничивает модель. 

\begin{figure}[H]
	\center{\includegraphics[width=0.7\linewidth]{imgs/ssa/gru}}
	
	синий -- GRU, оранжевый -- SSA-GRU
	\caption{Indian Rain GRU: p-value для GRU: 0.0202, p-value для SSA-GRU: 0.0118.}
	\label{res_rain_gru_ssa_series}
\end{figure}

\subsubsection{Выводы}

Из результатов становится понятно, что использование предобработки SSA приводит к уменьшению ошибки на реальных данных, рассматриваемых в этой главе.

\chapter*{Заключение}
\addcontentsline{toc}{chapter}{Заключение}

В работе был рассмотрена методика исследования сравнения методов, которая показалась весьма успешной, особенно на реальных данных среднемесячных осадков в Индии. В дальнейшем планируется ее развивать, в частности попробовать перенести все полученные результаты на данные параметров вращения Земли (EOP). Возможно придется усложнить модельные данные для данных EOP.

\begin{thebibliography}{1}
	\bibitem{SSA}
	Golyandina, N., Nekrutkin, V., \& Zhigljavsky, A. (2001). Analysis of time series structure: SSA and related techniques. Chapman \& Hall/CRC.
	\bibitem{ar1}
	Kongchang Du, Ying Zhao, Jiaqiang Lei (2017). \href{https://www.sciencedirect.com/science/article/abs/pii/S0022169417304237}{The incorrect usage of singular spectral analysis and discrete wavelet transform in hybrid models to predict hydrological time series.}
\end{thebibliography}

\end{document}

